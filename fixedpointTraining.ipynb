{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "#from utee import misc, quant, selector\n",
    "\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Load CIFAR-10\n",
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "def test_flatten():\n",
    "    x = torch.arange(12).view(2, 1, 3, 2)\n",
    "    print('Before flattening: ', x)\n",
    "    print('After flattening: ', flatten(x))\n",
    "\n",
    "#test_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    t_begin = time.time()\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            if t % print_every == 0:\n",
    "                t_elapse = time.time() - t_begin\n",
    "                print('Elapsed %.4f s, Epoch %d,  Iteration %d, loss = %.4f' % (t_elapse, e, t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), '../pretrain_model/training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Set up the layers you need for a three-layer ConvNet with the  #\n",
    "        # architecture defined above.                                          #\n",
    "        ########################################################################\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, stride=1, padding=2)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.constant_(self.conv1.bias, 0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.constant_(self.conv2.bias, 0)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64*8*8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "        ########################################################################\n",
    "        #                          END OF YOUR CODE                            #       \n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward function for a 3-layer ConvNet. you      #\n",
    "        # should use the layers you defined in __init__ and specify the        #\n",
    "        # connectivity of those layers in forward()                            #\n",
    "        ########################################################################\n",
    "        x = self.conv1(x)\n",
    "        #x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = self.conv2(x)\n",
    "        #x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        scores = x\n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.quantization import *\n",
    "\n",
    "class FixedLayerConvNet(nn.Module):\n",
    "    def __init__(self, _bits=8):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Set up the layers you need for a three-layer ConvNet with the  #\n",
    "        # architecture defined above.                                          #\n",
    "        ########################################################################\n",
    "        self.bits = _bits\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, stride=1, padding=2)\n",
    "        #nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        #nn.init.constant_(self.conv1.bias, 0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        #nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        #nn.init.constant_(self.conv2.bias, 0)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        #self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.fc1 = nn.Linear(64*8*8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        #nn.init.kaiming_normal_(self.fc.weight)\n",
    "        self.quant = activation_quantization(self.bits, Quant.linear)\n",
    "        ########################################################################\n",
    "        #                          END OF YOUR CODE                            #       \n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward function for a 3-layer ConvNet. you      #\n",
    "        # should use the layers you defined in __init__ and specify the        #\n",
    "        # connectivity of those layers in forward()                            #\n",
    "        ########################################################################\n",
    "        #x = quantization.apply(x, self.bits, Quant.linear)\n",
    "        x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        #x = quantization.apply(x, self.bits, Quant.linear)\n",
    "        x = self.quant(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = flatten(x)\n",
    "        x = self.quant(x)\n",
    "        #x = quantization.apply(x, self.bits, Quant.linear)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.quant(x)\n",
    "        #x = quantization.apply(x, self.bits, Quant.linear)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        scores = x\n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.quantization import *\n",
    "\n",
    "#TODO: 1. Implement different function\n",
    "#      2. differentiate FC and CONV\n",
    "def quantize_weight(model, bits):\n",
    "    weight = model.state_dict()\n",
    "    for k, v in weight.items():\n",
    "        weight[k] = Quant.linear(v, bits)\n",
    "        #print(weight[k])\n",
    "    model.load_state_dict(weight)\n",
    "    return model\n",
    "\n",
    "#TODO: Add a dictionary for bit width and function.\n",
    "def train_fixed_weight(model, optimizer, epochs=1, bits=8):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    t_begin = time.time()\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Add quantization for weight\n",
    "            model = quantize_weight(model, bits)\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                t_elapse = time.time() - t_begin\n",
    "                print('Elapsed %.4f s, Epoch %d,  Iteration %d, loss = %.4f' % (t_elapse, e, t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load module to loadmodel\n",
    "PATH = '../pretrain_model/training.pt'\n",
    "\n",
    "exp_bits = 4\n",
    "\n",
    "#define the model\n",
    "fix_model = FixedLayerConvNet(exp_bits)\n",
    "direct_fix_model = FixedLayerConvNet(exp_bits)\n",
    "model = ExpConvNet()\n",
    "\n",
    "\n",
    "#load model\n",
    "fix_model.load_state_dict(torch.load(PATH))\n",
    "fix_model = quantize_weight(fix_model, exp_bits)\n",
    "\n",
    "direct_fix_model.load_state_dict(torch.load(PATH))\n",
    "direct_fix_model = quantize_weight(direct_fix_model, exp_bits)\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#Solve the weight type problem, change to cudafloat tensor\n",
    "if USE_GPU:\n",
    "    fix_model.cuda()\n",
    "    fix_model = torch.nn.DataParallel(fix_model, device_ids=range(torch.cuda.device_count()))\n",
    "\n",
    "if USE_GPU:\n",
    "    direct_fix_model.cuda()\n",
    "    direct_fix_model = torch.nn.DataParallel(direct_fix_model, device_ids=range(torch.cuda.device_count()))\n",
    "    #cudnn.benchmark = True\n",
    "    \n",
    "if USE_GPU:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Floating Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 6950 / 10000 correct (69.50)\n",
      "Finetune Fixed Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 5423 / 10000 correct (54.23)\n",
      "\n",
      "Direct Fixed Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 5423 / 10000 correct (54.23)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOriginal Floating Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, model)\n",
    "print(\"Finetune Fixed Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, fix_model)\n",
    "print(\"\\nDirect Fixed Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, direct_fix_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 0.0152 s, Epoch 0,  Iteration 0, loss = 1.2426\n",
      "Checking accuracy on validation set\n",
      "Got 533 / 1000 correct (53.30)\n",
      "\n",
      "Elapsed 1.5043 s, Epoch 0,  Iteration 100, loss = 1.1681\n",
      "Checking accuracy on validation set\n",
      "Got 536 / 1000 correct (53.60)\n",
      "\n",
      "Elapsed 2.9968 s, Epoch 0,  Iteration 200, loss = 1.5309\n",
      "Checking accuracy on validation set\n",
      "Got 539 / 1000 correct (53.90)\n",
      "\n",
      "Elapsed 4.4826 s, Epoch 0,  Iteration 300, loss = 1.0697\n",
      "Checking accuracy on validation set\n",
      "Got 538 / 1000 correct (53.80)\n",
      "\n",
      "Elapsed 5.9684 s, Epoch 0,  Iteration 400, loss = 1.1535\n",
      "Checking accuracy on validation set\n",
      "Got 539 / 1000 correct (53.90)\n",
      "\n",
      "Elapsed 7.4563 s, Epoch 0,  Iteration 500, loss = 1.5188\n",
      "Checking accuracy on validation set\n",
      "Got 541 / 1000 correct (54.10)\n",
      "\n",
      "Elapsed 8.9535 s, Epoch 0,  Iteration 600, loss = 1.6893\n",
      "Checking accuracy on validation set\n",
      "Got 545 / 1000 correct (54.50)\n",
      "\n",
      "Elapsed 10.4472 s, Epoch 0,  Iteration 700, loss = 1.1961\n",
      "Checking accuracy on validation set\n",
      "Got 572 / 1000 correct (57.20)\n",
      "\n",
      "Elapsed 11.4813 s, Epoch 1,  Iteration 0, loss = 0.8937\n",
      "Checking accuracy on validation set\n",
      "Got 588 / 1000 correct (58.80)\n",
      "\n",
      "Elapsed 12.9719 s, Epoch 1,  Iteration 100, loss = 0.9114\n",
      "Checking accuracy on validation set\n",
      "Got 586 / 1000 correct (58.60)\n",
      "\n",
      "Elapsed 14.4571 s, Epoch 1,  Iteration 200, loss = 1.0881\n",
      "Checking accuracy on validation set\n",
      "Got 583 / 1000 correct (58.30)\n",
      "\n",
      "Elapsed 15.9498 s, Epoch 1,  Iteration 300, loss = 1.2819\n",
      "Checking accuracy on validation set\n",
      "Got 576 / 1000 correct (57.60)\n",
      "\n",
      "Elapsed 17.4439 s, Epoch 1,  Iteration 400, loss = 1.0092\n",
      "Checking accuracy on validation set\n",
      "Got 579 / 1000 correct (57.90)\n",
      "\n",
      "Elapsed 18.9394 s, Epoch 1,  Iteration 500, loss = 0.8934\n",
      "Checking accuracy on validation set\n",
      "Got 583 / 1000 correct (58.30)\n",
      "\n",
      "Elapsed 20.4384 s, Epoch 1,  Iteration 600, loss = 1.0932\n",
      "Checking accuracy on validation set\n",
      "Got 580 / 1000 correct (58.00)\n",
      "\n",
      "Elapsed 21.9304 s, Epoch 1,  Iteration 700, loss = 0.7083\n",
      "Checking accuracy on validation set\n",
      "Got 588 / 1000 correct (58.80)\n",
      "\n",
      "Elapsed 22.9644 s, Epoch 2,  Iteration 0, loss = 1.1619\n",
      "Checking accuracy on validation set\n",
      "Got 591 / 1000 correct (59.10)\n",
      "\n",
      "Elapsed 24.4675 s, Epoch 2,  Iteration 100, loss = 1.2551\n",
      "Checking accuracy on validation set\n",
      "Got 582 / 1000 correct (58.20)\n",
      "\n",
      "Elapsed 25.9623 s, Epoch 2,  Iteration 200, loss = 1.6262\n",
      "Checking accuracy on validation set\n",
      "Got 578 / 1000 correct (57.80)\n",
      "\n",
      "Elapsed 27.4540 s, Epoch 2,  Iteration 300, loss = 0.9046\n",
      "Checking accuracy on validation set\n",
      "Got 587 / 1000 correct (58.70)\n",
      "\n",
      "Elapsed 28.9403 s, Epoch 2,  Iteration 400, loss = 0.8912\n",
      "Checking accuracy on validation set\n",
      "Got 594 / 1000 correct (59.40)\n",
      "\n",
      "Elapsed 30.4353 s, Epoch 2,  Iteration 500, loss = 1.1462\n",
      "Checking accuracy on validation set\n",
      "Got 586 / 1000 correct (58.60)\n",
      "\n",
      "Elapsed 31.9336 s, Epoch 2,  Iteration 600, loss = 0.8654\n",
      "Checking accuracy on validation set\n",
      "Got 590 / 1000 correct (59.00)\n",
      "\n",
      "Elapsed 33.4223 s, Epoch 2,  Iteration 700, loss = 0.9004\n",
      "Checking accuracy on validation set\n",
      "Got 591 / 1000 correct (59.10)\n",
      "\n",
      "Elapsed 34.4542 s, Epoch 3,  Iteration 0, loss = 1.0644\n",
      "Checking accuracy on validation set\n",
      "Got 591 / 1000 correct (59.10)\n",
      "\n",
      "Elapsed 35.9421 s, Epoch 3,  Iteration 100, loss = 0.4786\n",
      "Checking accuracy on validation set\n",
      "Got 588 / 1000 correct (58.80)\n",
      "\n",
      "Elapsed 37.4324 s, Epoch 3,  Iteration 200, loss = 0.8750\n",
      "Checking accuracy on validation set\n",
      "Got 594 / 1000 correct (59.40)\n",
      "\n",
      "Elapsed 38.9293 s, Epoch 3,  Iteration 300, loss = 0.8809\n",
      "Checking accuracy on validation set\n",
      "Got 592 / 1000 correct (59.20)\n",
      "\n",
      "Elapsed 40.4194 s, Epoch 3,  Iteration 400, loss = 0.8884\n",
      "Checking accuracy on validation set\n",
      "Got 591 / 1000 correct (59.10)\n",
      "\n",
      "Elapsed 41.9106 s, Epoch 3,  Iteration 500, loss = 1.1111\n",
      "Checking accuracy on validation set\n",
      "Got 592 / 1000 correct (59.20)\n",
      "\n",
      "Elapsed 43.4097 s, Epoch 3,  Iteration 600, loss = 1.2006\n",
      "Checking accuracy on validation set\n",
      "Got 592 / 1000 correct (59.20)\n",
      "\n",
      "Elapsed 44.9060 s, Epoch 3,  Iteration 700, loss = 1.1181\n",
      "Checking accuracy on validation set\n",
      "Got 598 / 1000 correct (59.80)\n",
      "\n",
      "Elapsed 45.9396 s, Epoch 4,  Iteration 0, loss = 0.9770\n",
      "Checking accuracy on validation set\n",
      "Got 598 / 1000 correct (59.80)\n",
      "\n",
      "Elapsed 47.4339 s, Epoch 4,  Iteration 100, loss = 0.9654\n",
      "Checking accuracy on validation set\n",
      "Got 593 / 1000 correct (59.30)\n",
      "\n",
      "Elapsed 48.9273 s, Epoch 4,  Iteration 200, loss = 0.7620\n",
      "Checking accuracy on validation set\n",
      "Got 590 / 1000 correct (59.00)\n",
      "\n",
      "Elapsed 50.4245 s, Epoch 4,  Iteration 300, loss = 0.6637\n",
      "Checking accuracy on validation set\n",
      "Got 600 / 1000 correct (60.00)\n",
      "\n",
      "Elapsed 51.9231 s, Epoch 4,  Iteration 400, loss = 0.9946\n",
      "Checking accuracy on validation set\n",
      "Got 602 / 1000 correct (60.20)\n",
      "\n",
      "Elapsed 53.4166 s, Epoch 4,  Iteration 500, loss = 0.8298\n",
      "Checking accuracy on validation set\n",
      "Got 598 / 1000 correct (59.80)\n",
      "\n",
      "Elapsed 54.9105 s, Epoch 4,  Iteration 600, loss = 1.0298\n",
      "Checking accuracy on validation set\n",
      "Got 602 / 1000 correct (60.20)\n",
      "\n",
      "Elapsed 56.4077 s, Epoch 4,  Iteration 700, loss = 0.8430\n",
      "Checking accuracy on validation set\n",
      "Got 600 / 1000 correct (60.00)\n",
      "\n",
      "Elapsed 57.4371 s, Epoch 5,  Iteration 0, loss = 0.5704\n",
      "Checking accuracy on validation set\n",
      "Got 610 / 1000 correct (61.00)\n",
      "\n",
      "Elapsed 58.9264 s, Epoch 5,  Iteration 100, loss = 1.0340\n",
      "Checking accuracy on validation set\n",
      "Got 595 / 1000 correct (59.50)\n",
      "\n",
      "Elapsed 60.4158 s, Epoch 5,  Iteration 200, loss = 1.2575\n",
      "Checking accuracy on validation set\n",
      "Got 604 / 1000 correct (60.40)\n",
      "\n",
      "Elapsed 61.9112 s, Epoch 5,  Iteration 300, loss = 0.8601\n",
      "Checking accuracy on validation set\n",
      "Got 594 / 1000 correct (59.40)\n",
      "\n",
      "Elapsed 63.4180 s, Epoch 5,  Iteration 400, loss = 0.6783\n",
      "Checking accuracy on validation set\n",
      "Got 601 / 1000 correct (60.10)\n",
      "\n",
      "Elapsed 64.9101 s, Epoch 5,  Iteration 500, loss = 1.1532\n",
      "Checking accuracy on validation set\n",
      "Got 598 / 1000 correct (59.80)\n",
      "\n",
      "Elapsed 66.4092 s, Epoch 5,  Iteration 600, loss = 0.9122\n",
      "Checking accuracy on validation set\n",
      "Got 593 / 1000 correct (59.30)\n",
      "\n",
      "Elapsed 67.9127 s, Epoch 5,  Iteration 700, loss = 0.8368\n",
      "Checking accuracy on validation set\n",
      "Got 601 / 1000 correct (60.10)\n",
      "\n",
      "Elapsed 68.9488 s, Epoch 6,  Iteration 0, loss = 0.7285\n",
      "Checking accuracy on validation set\n",
      "Got 595 / 1000 correct (59.50)\n",
      "\n",
      "Elapsed 70.4424 s, Epoch 6,  Iteration 100, loss = 1.1102\n",
      "Checking accuracy on validation set\n",
      "Got 599 / 1000 correct (59.90)\n",
      "\n",
      "Elapsed 71.9321 s, Epoch 6,  Iteration 200, loss = 0.6459\n",
      "Checking accuracy on validation set\n",
      "Got 590 / 1000 correct (59.00)\n",
      "\n",
      "Elapsed 73.4203 s, Epoch 6,  Iteration 300, loss = 1.0346\n",
      "Checking accuracy on validation set\n",
      "Got 588 / 1000 correct (58.80)\n",
      "\n",
      "Elapsed 74.9124 s, Epoch 6,  Iteration 400, loss = 1.4094\n",
      "Checking accuracy on validation set\n",
      "Got 592 / 1000 correct (59.20)\n",
      "\n",
      "Elapsed 76.4029 s, Epoch 6,  Iteration 500, loss = 0.9150\n",
      "Checking accuracy on validation set\n",
      "Got 596 / 1000 correct (59.60)\n",
      "\n",
      "Elapsed 77.8890 s, Epoch 6,  Iteration 600, loss = 0.8660\n",
      "Checking accuracy on validation set\n",
      "Got 592 / 1000 correct (59.20)\n",
      "\n",
      "Elapsed 79.3758 s, Epoch 6,  Iteration 700, loss = 0.6864\n",
      "Checking accuracy on validation set\n",
      "Got 593 / 1000 correct (59.30)\n",
      "\n",
      "Elapsed 80.4080 s, Epoch 7,  Iteration 0, loss = 0.6450\n",
      "Checking accuracy on validation set\n",
      "Got 605 / 1000 correct (60.50)\n",
      "\n",
      "Elapsed 81.8963 s, Epoch 7,  Iteration 100, loss = 0.5747\n",
      "Checking accuracy on validation set\n",
      "Got 593 / 1000 correct (59.30)\n",
      "\n",
      "Elapsed 83.3927 s, Epoch 7,  Iteration 200, loss = 0.8926\n",
      "Checking accuracy on validation set\n",
      "Got 608 / 1000 correct (60.80)\n",
      "\n",
      "Elapsed 84.8860 s, Epoch 7,  Iteration 300, loss = 0.9926\n",
      "Checking accuracy on validation set\n",
      "Got 596 / 1000 correct (59.60)\n",
      "\n",
      "Elapsed 86.3773 s, Epoch 7,  Iteration 400, loss = 0.6816\n",
      "Checking accuracy on validation set\n",
      "Got 601 / 1000 correct (60.10)\n",
      "\n",
      "Elapsed 87.8660 s, Epoch 7,  Iteration 500, loss = 0.7530\n",
      "Checking accuracy on validation set\n",
      "Got 594 / 1000 correct (59.40)\n",
      "\n",
      "Elapsed 89.3538 s, Epoch 7,  Iteration 600, loss = 0.7716\n",
      "Checking accuracy on validation set\n",
      "Got 600 / 1000 correct (60.00)\n",
      "\n",
      "Elapsed 90.8452 s, Epoch 7,  Iteration 700, loss = 0.5424\n",
      "Checking accuracy on validation set\n",
      "Got 598 / 1000 correct (59.80)\n",
      "\n",
      "Elapsed 91.8810 s, Epoch 8,  Iteration 0, loss = 0.5400\n",
      "Checking accuracy on validation set\n",
      "Got 600 / 1000 correct (60.00)\n",
      "\n",
      "Elapsed 93.3810 s, Epoch 8,  Iteration 100, loss = 0.8826\n",
      "Checking accuracy on validation set\n",
      "Got 595 / 1000 correct (59.50)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 94.8886 s, Epoch 8,  Iteration 200, loss = 0.6344\n",
      "Checking accuracy on validation set\n",
      "Got 603 / 1000 correct (60.30)\n",
      "\n",
      "Elapsed 96.4020 s, Epoch 8,  Iteration 300, loss = 0.9142\n",
      "Checking accuracy on validation set\n",
      "Got 598 / 1000 correct (59.80)\n",
      "\n",
      "Elapsed 97.9242 s, Epoch 8,  Iteration 400, loss = 1.0082\n",
      "Checking accuracy on validation set\n",
      "Got 602 / 1000 correct (60.20)\n",
      "\n",
      "Elapsed 99.4206 s, Epoch 8,  Iteration 500, loss = 0.7071\n",
      "Checking accuracy on validation set\n",
      "Got 602 / 1000 correct (60.20)\n",
      "\n",
      "Elapsed 100.9164 s, Epoch 8,  Iteration 600, loss = 0.9210\n",
      "Checking accuracy on validation set\n",
      "Got 599 / 1000 correct (59.90)\n",
      "\n",
      "Elapsed 102.4089 s, Epoch 8,  Iteration 700, loss = 1.0858\n",
      "Checking accuracy on validation set\n",
      "Got 595 / 1000 correct (59.50)\n",
      "\n",
      "Elapsed 103.4445 s, Epoch 9,  Iteration 0, loss = 1.3169\n",
      "Checking accuracy on validation set\n",
      "Got 602 / 1000 correct (60.20)\n",
      "\n",
      "Elapsed 104.9390 s, Epoch 9,  Iteration 100, loss = 1.0781\n",
      "Checking accuracy on validation set\n",
      "Got 594 / 1000 correct (59.40)\n",
      "\n",
      "Elapsed 106.4317 s, Epoch 9,  Iteration 200, loss = 1.0365\n",
      "Checking accuracy on validation set\n",
      "Got 595 / 1000 correct (59.50)\n",
      "\n",
      "Elapsed 107.9207 s, Epoch 9,  Iteration 300, loss = 0.7583\n",
      "Checking accuracy on validation set\n",
      "Got 603 / 1000 correct (60.30)\n",
      "\n",
      "Elapsed 109.4104 s, Epoch 9,  Iteration 400, loss = 0.6177\n",
      "Checking accuracy on validation set\n",
      "Got 591 / 1000 correct (59.10)\n",
      "\n",
      "Elapsed 110.8999 s, Epoch 9,  Iteration 500, loss = 0.9043\n",
      "Checking accuracy on validation set\n",
      "Got 599 / 1000 correct (59.90)\n",
      "\n",
      "Elapsed 112.3912 s, Epoch 9,  Iteration 600, loss = 1.1062\n",
      "Checking accuracy on validation set\n",
      "Got 598 / 1000 correct (59.80)\n",
      "\n",
      "Elapsed 113.8839 s, Epoch 9,  Iteration 700, loss = 0.6107\n",
      "Checking accuracy on validation set\n",
      "Got 598 / 1000 correct (59.80)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train this model\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = optim.Adam(params=fix_model.parameters(), lr=learning_rate)\n",
    "#optimizer = optim.SGD(fix_model.parameters(), lr=learning_rate,momentum=0.9, nesterov=True)\n",
    "train_fixed_weight(fix_model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Fixed Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 5959 / 10000 correct (59.59)\n",
      "\n",
      "Direct Fixed Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 5423 / 10000 correct (54.23)\n"
     ]
    }
   ],
   "source": [
    "print(\"Finetune Fixed Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, fix_model)\n",
    "print(\"\\nDirect Fixed Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, direct_fix_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  4.,  10.,  -2.,  12.,  10.],\n",
      "          [  2.,  -4.,  12.,  12.,   6.],\n",
      "          [  0.,  18.,   0.,  12.,  -8.],\n",
      "          [-18.,   0.,  -8., -10., -14.],\n",
      "          [-12., -22.,   0.,  -4.,  -6.]],\n",
      "\n",
      "         [[ -6., -14., -22.,   4.,  10.],\n",
      "          [  6.,   2.,  -4.,   6.,  -2.],\n",
      "          [  8.,  26.,  10.,   8.,   0.],\n",
      "          [ -4.,  12., -12., -12.,  -6.],\n",
      "          [  4.,   2.,  16., -24.,  -6.]],\n",
      "\n",
      "         [[  0.,  24.,  -6.,  24.,  22.],\n",
      "          [-16., -28.,  12.,  -2.,   0.],\n",
      "          [ -2.,   4.,   0.,  10.,   8.],\n",
      "          [ -2.,  -6.,  -4., -10.,  -8.],\n",
      "          [ -2.,  -6.,  -6.,   0.,  -6.]]],\n",
      "\n",
      "\n",
      "        [[[ -2.,   2.,   2.,  -2.,  -6.],\n",
      "          [ -2.,  -6.,  10.,   6.,   2.],\n",
      "          [ -2., -20.,  -4.,  10.,  46.],\n",
      "          [  8.,   2.,  -2.,   4.,  14.],\n",
      "          [ -2.,  14.,  -4., -30.,  -4.]],\n",
      "\n",
      "         [[  4.,   8.,  -8.,  10., -26.],\n",
      "          [-10., -12.,   6.,   6.,   8.],\n",
      "          [  4., -12.,  -2.,  10.,   2.],\n",
      "          [ 12.,   6.,  -6., -14.,  -4.],\n",
      "          [ -2.,   8.,   6.,  14., -12.]],\n",
      "\n",
      "         [[ 10.,  26.,  -6.,  12.,  -8.],\n",
      "          [  4., -26.,   0.,   0.,   0.],\n",
      "          [-12.,  16.,   6.,  -4.,  -4.],\n",
      "          [ -4.,   0., -20., -12.,  -8.],\n",
      "          [ 10.,   6.,  20.,   2.,  -8.]]],\n",
      "\n",
      "\n",
      "        [[[-20.,   6., -12.,  -2.,   8.],\n",
      "          [  8.,  -6.,  -6.,  14.,   8.],\n",
      "          [ 12., -26.,   6.,  12.,  -2.],\n",
      "          [  6., -12.,   6.,  16.,   4.],\n",
      "          [  0., -10.,  18.,  -2.,   2.]],\n",
      "\n",
      "         [[  0.,  10., -14., -30.,  14.],\n",
      "          [ 22., -10., -20.,  16.,   8.],\n",
      "          [ -4.,   4.,  -4.,  14.,   2.],\n",
      "          [  0., -10.,  18.,  -2.,   0.],\n",
      "          [-20.,  -4., -10.,   4.,   6.]],\n",
      "\n",
      "         [[ -8.,   4., -10.,   0.,  -2.],\n",
      "          [ 12., -18.,  -6.,  16.,  10.],\n",
      "          [ -4.,   2.,  14., -12.,  -6.],\n",
      "          [ -2.,  -4.,   8.,   2.,   2.],\n",
      "          [ -4.,   8.,   6.,  -8.,  -6.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-20.,   0.,  12.,  -4.,  -6.],\n",
      "          [-10.,  -2.,  -2.,   2.,   8.],\n",
      "          [-18.,  -6.,  28.,  16., -28.],\n",
      "          [ -6., -14.,  30.,   4., -12.],\n",
      "          [-24.,  -4.,  18., -10.,  -2.]],\n",
      "\n",
      "         [[ -8.,   4.,   2.,   8.,  10.],\n",
      "          [ -6.,  -2.,  14.,   6.,   6.],\n",
      "          [-18.,  -4.,  16.,  -2.,  -4.],\n",
      "          [-14.,  10.,  -2.,  12.,   0.],\n",
      "          [-10., -22.,  14.,  18.,   2.]],\n",
      "\n",
      "         [[  8.,   0., -16.,   6.,  10.],\n",
      "          [ -4.,  10.,   0.,  -6.,  -4.],\n",
      "          [ -6.,  -2.,  -2., -14.,  18.],\n",
      "          [-16.,   0.,  -8.,  22., -12.],\n",
      "          [  2.,   2.,  -8.,   6.,  10.]]],\n",
      "\n",
      "\n",
      "        [[[ -2.,   8.,  12.,  -4.,   6.],\n",
      "          [  0.,   6.,   4.,  -8.,  -8.],\n",
      "          [-16.,  20.,   6.,  12., -10.],\n",
      "          [-24., -16.,  18.,  -2.,   0.],\n",
      "          [ -6.,   4., -20.,  20., -10.]],\n",
      "\n",
      "         [[ -6.,   2.,  -2.,  -2.,   0.],\n",
      "          [ -2.,  18.,   2.,  -8., -10.],\n",
      "          [  4.,   0.,  10.,  16., -20.],\n",
      "          [ -6.,  10.,  14.,  12.,   4.],\n",
      "          [ -6.,   4.,  -2.,   6.,  20.]],\n",
      "\n",
      "         [[ -6.,   2.,  -4., -18.,   0.],\n",
      "          [  8.,   2.,   2.,  -6., -14.],\n",
      "          [ -4.,   2.,  30.,  -2.,   2.],\n",
      "          [-14.,  -6.,   0.,  16., -12.],\n",
      "          [-10.,  -8.,  -4.,  -4.,  -4.]]],\n",
      "\n",
      "\n",
      "        [[[  2.,  -2., -26., -12.,  20.],\n",
      "          [ 16.,  12., -16.,   8.,   0.],\n",
      "          [  0., -10.,   4.,   2., -10.],\n",
      "          [  2., -12.,  12.,   4.,   2.],\n",
      "          [ 10.,  -8.,  10.,  -4.,  -6.]],\n",
      "\n",
      "         [[ 18., -12.,  -4.,   4.,  32.],\n",
      "          [ 36., -12., -12., -14.,  18.],\n",
      "          [  8., -10., -28.,  -8.,  20.],\n",
      "          [  8., -18., -14.,  22.,  -6.],\n",
      "          [  0.,   6.,  -4.,  -8.,   6.]],\n",
      "\n",
      "         [[  8.,  -6., -18., -16.,  12.],\n",
      "          [ 38.,  14., -18.,   2.,   0.],\n",
      "          [ 12., -12.,  -2., -24.,   6.],\n",
      "          [ -4.,  -4., -12.,  32.,  -4.],\n",
      "          [ -6.,   6.,  -4., -12.,  -2.]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(direct_fix_model.module.state_dict()['conv1.weight']*2**(exp_bits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
