{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from utee import misc, quant, selector\n",
    "\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Load CIFAR-10\n",
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "def test_flatten():\n",
    "    x = torch.arange(12).view(2, 1, 3, 2)\n",
    "    print('Before flattening: ', x)\n",
    "    print('After flattening: ', flatten(x))\n",
    "\n",
    "#test_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    t_begin = time.time()\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            if t % print_every == 0:\n",
    "                t_elapse = time.time() - t_begin\n",
    "                print('Elapsed %.4f s, Epoch %d,  Iteration %d, loss = %.4f' % (t_elapse, e, t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #         \n",
    "# Experiment with any architectures, optimizers, and hyperparameters.          #\n",
    "# Achieve AT LEAST 70% accuracy on the *validation set* within 10 epochs.      #\n",
    "#                                                                              #\n",
    "# Note that you can use the check_accuracy function to evaluate on either      #\n",
    "# the test set or the validation set, by passing either loader_test or         #\n",
    "# loader_val as the second argument to check_accuracy. You should not touch    #\n",
    "# the test set until you have finished your architecture and  hyperparameter   #\n",
    "# tuning, and only run the test set once at the end to report a final value.   #\n",
    "################################################################################\n",
    "\n",
    "'''\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 5,stride=1, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "    nn.Conv2d(32, 64, 3,stride=1,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "    Flatten(),\n",
    "    nn.Linear(4096,512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512,10),\n",
    ")\n",
    "'''\n",
    "\n",
    "class ExpConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Set up the layers you need for a three-layer ConvNet with the  #\n",
    "        # architecture defined above.                                          #\n",
    "        ########################################################################\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, stride=1, padding=2)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.constant_(self.conv1.bias, 0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.constant_(self.conv2.bias, 0)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64*8*8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "        ########################################################################\n",
    "        #                          END OF YOUR CODE                            #       \n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward function for a 3-layer ConvNet. you      #\n",
    "        # should use the layers you defined in __init__ and specify the        #\n",
    "        # connectivity of those layers in forward()                            #\n",
    "        ########################################################################\n",
    "        x = self.conv1(x)\n",
    "        #x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = self.conv2(x)\n",
    "        #x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        scores = x\n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 0.5982 s, Epoch 0,  Iteration 0, loss = 3.9637\n",
      "Checking accuracy on validation set\n",
      "Got 119 / 1000 correct (11.90)\n",
      "\n",
      "Elapsed 2.1070 s, Epoch 0,  Iteration 100, loss = 1.5728\n",
      "Checking accuracy on validation set\n",
      "Got 411 / 1000 correct (41.10)\n",
      "\n",
      "Elapsed 3.4613 s, Epoch 0,  Iteration 200, loss = 1.4517\n",
      "Checking accuracy on validation set\n",
      "Got 471 / 1000 correct (47.10)\n",
      "\n",
      "Elapsed 4.8255 s, Epoch 0,  Iteration 300, loss = 1.4261\n",
      "Checking accuracy on validation set\n",
      "Got 534 / 1000 correct (53.40)\n",
      "\n",
      "Elapsed 6.1987 s, Epoch 0,  Iteration 400, loss = 1.1204\n",
      "Checking accuracy on validation set\n",
      "Got 566 / 1000 correct (56.60)\n",
      "\n",
      "Elapsed 7.5706 s, Epoch 0,  Iteration 500, loss = 1.1241\n",
      "Checking accuracy on validation set\n",
      "Got 583 / 1000 correct (58.30)\n",
      "\n",
      "Elapsed 8.9577 s, Epoch 0,  Iteration 600, loss = 1.2526\n",
      "Checking accuracy on validation set\n",
      "Got 593 / 1000 correct (59.30)\n",
      "\n",
      "Elapsed 10.3222 s, Epoch 0,  Iteration 700, loss = 1.1940\n",
      "Checking accuracy on validation set\n",
      "Got 616 / 1000 correct (61.60)\n",
      "\n",
      "Elapsed 11.3051 s, Epoch 1,  Iteration 0, loss = 1.1668\n",
      "Checking accuracy on validation set\n",
      "Got 611 / 1000 correct (61.10)\n",
      "\n",
      "Elapsed 12.6732 s, Epoch 1,  Iteration 100, loss = 0.8468\n",
      "Checking accuracy on validation set\n",
      "Got 622 / 1000 correct (62.20)\n",
      "\n",
      "Elapsed 14.0481 s, Epoch 1,  Iteration 200, loss = 0.9571\n",
      "Checking accuracy on validation set\n",
      "Got 635 / 1000 correct (63.50)\n",
      "\n",
      "Elapsed 15.4244 s, Epoch 1,  Iteration 300, loss = 1.0384\n",
      "Checking accuracy on validation set\n",
      "Got 645 / 1000 correct (64.50)\n",
      "\n",
      "Elapsed 16.8024 s, Epoch 1,  Iteration 400, loss = 1.0715\n",
      "Checking accuracy on validation set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "\n",
      "Elapsed 18.1758 s, Epoch 1,  Iteration 500, loss = 0.7345\n",
      "Checking accuracy on validation set\n",
      "Got 641 / 1000 correct (64.10)\n",
      "\n",
      "Elapsed 19.5624 s, Epoch 1,  Iteration 600, loss = 0.9044\n",
      "Checking accuracy on validation set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "\n",
      "Elapsed 20.9815 s, Epoch 1,  Iteration 700, loss = 0.8505\n",
      "Checking accuracy on validation set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "\n",
      "Elapsed 21.9834 s, Epoch 2,  Iteration 0, loss = 0.7957\n",
      "Checking accuracy on validation set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "\n",
      "Elapsed 23.3548 s, Epoch 2,  Iteration 100, loss = 0.6227\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 24.7434 s, Epoch 2,  Iteration 200, loss = 0.4842\n",
      "Checking accuracy on validation set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "\n",
      "Elapsed 26.1332 s, Epoch 2,  Iteration 300, loss = 0.4793\n",
      "Checking accuracy on validation set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "\n",
      "Elapsed 27.5186 s, Epoch 2,  Iteration 400, loss = 0.7945\n",
      "Checking accuracy on validation set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "\n",
      "Elapsed 28.8977 s, Epoch 2,  Iteration 500, loss = 0.6899\n",
      "Checking accuracy on validation set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "\n",
      "Elapsed 30.2732 s, Epoch 2,  Iteration 600, loss = 0.6771\n",
      "Checking accuracy on validation set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "\n",
      "Elapsed 31.6529 s, Epoch 2,  Iteration 700, loss = 0.6344\n",
      "Checking accuracy on validation set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "\n",
      "Elapsed 32.6255 s, Epoch 3,  Iteration 0, loss = 0.5280\n",
      "Checking accuracy on validation set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "\n",
      "Elapsed 34.0038 s, Epoch 3,  Iteration 100, loss = 0.4608\n",
      "Checking accuracy on validation set\n",
      "Got 730 / 1000 correct (73.00)\n",
      "\n",
      "Elapsed 35.3653 s, Epoch 3,  Iteration 200, loss = 0.5055\n",
      "Checking accuracy on validation set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "\n",
      "Elapsed 36.7438 s, Epoch 3,  Iteration 300, loss = 0.5661\n",
      "Checking accuracy on validation set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "\n",
      "Elapsed 38.1206 s, Epoch 3,  Iteration 400, loss = 0.5268\n",
      "Checking accuracy on validation set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "\n",
      "Elapsed 39.4993 s, Epoch 3,  Iteration 500, loss = 0.5840\n",
      "Checking accuracy on validation set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "\n",
      "Elapsed 40.8755 s, Epoch 3,  Iteration 600, loss = 0.4976\n",
      "Checking accuracy on validation set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "\n",
      "Elapsed 42.2367 s, Epoch 3,  Iteration 700, loss = 0.4120\n",
      "Checking accuracy on validation set\n",
      "Got 714 / 1000 correct (71.40)\n",
      "\n",
      "Elapsed 43.2022 s, Epoch 4,  Iteration 0, loss = 0.2708\n",
      "Checking accuracy on validation set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "\n",
      "Elapsed 44.5731 s, Epoch 4,  Iteration 100, loss = 0.1354\n",
      "Checking accuracy on validation set\n",
      "Got 736 / 1000 correct (73.60)\n",
      "\n",
      "Elapsed 45.9339 s, Epoch 4,  Iteration 200, loss = 0.2157\n",
      "Checking accuracy on validation set\n",
      "Got 712 / 1000 correct (71.20)\n",
      "\n",
      "Elapsed 47.2945 s, Epoch 4,  Iteration 300, loss = 0.2548\n",
      "Checking accuracy on validation set\n",
      "Got 712 / 1000 correct (71.20)\n",
      "\n",
      "Elapsed 48.6593 s, Epoch 4,  Iteration 400, loss = 0.2453\n",
      "Checking accuracy on validation set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "\n",
      "Elapsed 50.0139 s, Epoch 4,  Iteration 500, loss = 0.3113\n",
      "Checking accuracy on validation set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "\n",
      "Elapsed 51.3695 s, Epoch 4,  Iteration 600, loss = 0.3595\n",
      "Checking accuracy on validation set\n",
      "Got 718 / 1000 correct (71.80)\n",
      "\n",
      "Elapsed 52.7463 s, Epoch 4,  Iteration 700, loss = 0.3079\n",
      "Checking accuracy on validation set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ExpConvNet()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "# You should get at least 70% accuracy\n",
    "train_part34(model, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finish define the linear quantization function, \n",
    "#next to figure out how to copy data from the orginal model to the new model\n",
    "\n",
    "#TODO: ADD more quantization function\n",
    "import math\n",
    "\n",
    "class Quant:\n",
    "    def linear(input, bits):\n",
    "        assert bits >= 1, bits\n",
    "        if bits == 1:\n",
    "            return torch.sign(input) - 1     \n",
    "        sf = torch.ceil(torch.log2(torch.max(torch.abs(input))))\n",
    "        delta = math.pow(2.0, -sf)\n",
    "        bound = math.pow(2.0, bits-1)\n",
    "        min_val = - bound\n",
    "        max_val = bound - 1\n",
    "        rounded = torch.floor(input / delta)\n",
    "\n",
    "        clipped_value = torch.clamp(rounded, min_val, max_val) * delta\n",
    "        return clipped_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class quantization(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  We can implement our own custom autograd Functions by subclassing\n",
    "  torch.autograd.Function and implementing the forward and backward passes\n",
    "  which operate on Tensors.\n",
    "  \"\"\"\n",
    "  @staticmethod\n",
    "  def forward(ctx, x, bits, quant_func):\n",
    "    \"\"\"\n",
    "    In the forward pass we receive a context object and a Tensor containing the\n",
    "    input; we must return a Tensor containing the output, and we can use the\n",
    "    context object to cache objects for use in the backward pass.\n",
    "    \"\"\"\n",
    "    #Define a constant\n",
    "    ctx.bits = bits\n",
    "    #ctx.save_for_backward(x)\n",
    "    clipped_value = quant_func(x, bits)\n",
    "    return clipped_value\n",
    "\n",
    "  def backward(ctx, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass we receive the context object and a Tensor containing\n",
    "    the gradient of the loss with respect to the output produced during the\n",
    "    forward pass. We can retrieve cached data from the context object, and must\n",
    "    compute and return the gradient of the loss with respect to the input to the\n",
    "    forward function.\n",
    "    \"\"\"\n",
    "    grad_x = grad_output.clone()\n",
    "    return grad_x, None, None\n",
    "\n",
    "class activation_quantization(nn.Module):\n",
    "    def __init__(self, bits=8, quant_func=Quant.linear):\n",
    "        super(activation_quantization, self).__init__()\n",
    "        self.bits = bits\n",
    "        self.func = quant_func\n",
    "        \n",
    "    def forward(self, inputActivation):\n",
    "        return quantization.apply(inputActivation, self.bits, self.func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedLayerConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Set up the layers you need for a three-layer ConvNet with the  #\n",
    "        # architecture defined above.                                          #\n",
    "        ########################################################################\n",
    "        self.bits = 8\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, stride=1, padding=2)\n",
    "        #nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        #nn.init.constant_(self.conv1.bias, 0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        #nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        #nn.init.constant_(self.conv2.bias, 0)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        #self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.fc1 = nn.Linear(64*8*8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        #nn.init.kaiming_normal_(self.fc.weight)\n",
    "        self.quant = activation_quantization(8, Quant.linear)\n",
    "        ########################################################################\n",
    "        #                          END OF YOUR CODE                            #       \n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward function for a 3-layer ConvNet. you      #\n",
    "        # should use the layers you defined in __init__ and specify the        #\n",
    "        # connectivity of those layers in forward()                            #\n",
    "        ########################################################################\n",
    "        #x = quantization.apply(x, self.bits, Quant.linear)\n",
    "        x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        #x = quantization.apply(x, self.bits, Quant.linear)\n",
    "        x = self.quant(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = flatten(x)\n",
    "        x = self.quant(x)\n",
    "        #x = quantization.apply(x, self.bits, Quant.linear)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.quant(x)\n",
    "        #x = quantization.apply(x, self.bits, Quant.linear)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        scores = x\n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################\n",
    "        return scores\n",
    "\n",
    "fix_model = FixedLayerConvNet()\n",
    "direct_fix_model = FixedLayerConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FixedLayerConvNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (quant): activation_quantization()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'training.pt'\n",
    "model_pretrain = torch.load(PATH)\n",
    "fix_model\n",
    "#model_pretrain\n",
    "#for k, v in model.module.state_dict().items():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load module to loadmodel\n",
    "PATH = 'training.pt'\n",
    "fix_model.load_state_dict(torch.load(PATH))\n",
    "direct_fix_model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#Solve the weight type problem, change to cudafloat tensor\n",
    "if USE_GPU:\n",
    "    fix_model.cuda()\n",
    "    fix_model = torch.nn.DataParallel(fix_model, device_ids=range(torch.cuda.device_count()))\n",
    "\n",
    "if USE_GPU:\n",
    "    direct_fix_model.cuda()\n",
    "    direct_fix_model = torch.nn.DataParallel(direct_fix_model, device_ids=range(torch.cuda.device_count()))\n",
    "    #cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 0.2790 s, Epoch 0,  Iteration 0, loss = 0.3178\n",
      "Checking accuracy on validation set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "\n",
      "Elapsed 2.1112 s, Epoch 0,  Iteration 100, loss = 0.4158\n",
      "Checking accuracy on validation set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "\n",
      "Elapsed 3.9778 s, Epoch 0,  Iteration 200, loss = 0.4012\n",
      "Checking accuracy on validation set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "\n",
      "Elapsed 5.8192 s, Epoch 0,  Iteration 300, loss = 0.3750\n",
      "Checking accuracy on validation set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "\n",
      "Elapsed 7.6737 s, Epoch 0,  Iteration 400, loss = 0.3526\n",
      "Checking accuracy on validation set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "\n",
      "Elapsed 9.5409 s, Epoch 0,  Iteration 500, loss = 0.2438\n",
      "Checking accuracy on validation set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "\n",
      "Elapsed 11.4260 s, Epoch 0,  Iteration 600, loss = 0.2283\n",
      "Checking accuracy on validation set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "\n",
      "Elapsed 13.2822 s, Epoch 0,  Iteration 700, loss = 0.3475\n",
      "Checking accuracy on validation set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "\n",
      "Elapsed 14.5811 s, Epoch 1,  Iteration 0, loss = 0.2491\n",
      "Checking accuracy on validation set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "\n",
      "Elapsed 16.4218 s, Epoch 1,  Iteration 100, loss = 0.3310\n",
      "Checking accuracy on validation set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "\n",
      "Elapsed 18.2812 s, Epoch 1,  Iteration 200, loss = 0.2608\n",
      "Checking accuracy on validation set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "\n",
      "Elapsed 20.1653 s, Epoch 1,  Iteration 300, loss = 0.2954\n",
      "Checking accuracy on validation set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "\n",
      "Elapsed 22.0501 s, Epoch 1,  Iteration 400, loss = 0.2579\n",
      "Checking accuracy on validation set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "\n",
      "Elapsed 23.9266 s, Epoch 1,  Iteration 500, loss = 0.2811\n",
      "Checking accuracy on validation set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "\n",
      "Elapsed 25.8293 s, Epoch 1,  Iteration 600, loss = 0.2345\n",
      "Checking accuracy on validation set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "\n",
      "Elapsed 27.7457 s, Epoch 1,  Iteration 700, loss = 0.2296\n",
      "Checking accuracy on validation set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "\n",
      "Elapsed 29.0684 s, Epoch 2,  Iteration 0, loss = 0.1920\n",
      "Checking accuracy on validation set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "\n",
      "Elapsed 30.9861 s, Epoch 2,  Iteration 100, loss = 0.3081\n",
      "Checking accuracy on validation set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "\n",
      "Elapsed 32.8838 s, Epoch 2,  Iteration 200, loss = 0.2276\n",
      "Checking accuracy on validation set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "\n",
      "Elapsed 34.7525 s, Epoch 2,  Iteration 300, loss = 0.2668\n",
      "Checking accuracy on validation set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "\n",
      "Elapsed 36.5696 s, Epoch 2,  Iteration 400, loss = 0.2093\n",
      "Checking accuracy on validation set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "\n",
      "Elapsed 38.3908 s, Epoch 2,  Iteration 500, loss = 0.2668\n",
      "Checking accuracy on validation set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "\n",
      "Elapsed 40.1870 s, Epoch 2,  Iteration 600, loss = 0.2606\n",
      "Checking accuracy on validation set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "\n",
      "Elapsed 41.9509 s, Epoch 2,  Iteration 700, loss = 0.2358\n",
      "Checking accuracy on validation set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "\n",
      "Elapsed 43.1995 s, Epoch 3,  Iteration 0, loss = 0.2615\n",
      "Checking accuracy on validation set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "\n",
      "Elapsed 45.0147 s, Epoch 3,  Iteration 100, loss = 0.4707\n",
      "Checking accuracy on validation set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "\n",
      "Elapsed 46.8444 s, Epoch 3,  Iteration 200, loss = 0.1313\n",
      "Checking accuracy on validation set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "\n",
      "Elapsed 48.6744 s, Epoch 3,  Iteration 300, loss = 0.2056\n",
      "Checking accuracy on validation set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "\n",
      "Elapsed 50.4997 s, Epoch 3,  Iteration 400, loss = 0.5164\n",
      "Checking accuracy on validation set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "\n",
      "Elapsed 52.3543 s, Epoch 3,  Iteration 500, loss = 0.1858\n",
      "Checking accuracy on validation set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "\n",
      "Elapsed 54.2000 s, Epoch 3,  Iteration 600, loss = 0.2391\n",
      "Checking accuracy on validation set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "\n",
      "Elapsed 56.1399 s, Epoch 3,  Iteration 700, loss = 0.2595\n",
      "Checking accuracy on validation set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "\n",
      "Elapsed 57.4345 s, Epoch 4,  Iteration 0, loss = 0.2633\n",
      "Checking accuracy on validation set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "\n",
      "Elapsed 59.3044 s, Epoch 4,  Iteration 100, loss = 0.1805\n",
      "Checking accuracy on validation set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "\n",
      "Elapsed 61.1546 s, Epoch 4,  Iteration 200, loss = 0.2184\n",
      "Checking accuracy on validation set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "\n",
      "Elapsed 63.0474 s, Epoch 4,  Iteration 300, loss = 0.2043\n",
      "Checking accuracy on validation set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "\n",
      "Elapsed 64.9104 s, Epoch 4,  Iteration 400, loss = 0.3229\n",
      "Checking accuracy on validation set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "\n",
      "Elapsed 66.7597 s, Epoch 4,  Iteration 500, loss = 0.1935\n",
      "Checking accuracy on validation set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "\n",
      "Elapsed 68.6012 s, Epoch 4,  Iteration 600, loss = 0.1895\n",
      "Checking accuracy on validation set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "\n",
      "Elapsed 70.4552 s, Epoch 4,  Iteration 700, loss = 0.1810\n",
      "Checking accuracy on validation set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "\n",
      "Elapsed 71.7553 s, Epoch 5,  Iteration 0, loss = 0.2841\n",
      "Checking accuracy on validation set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "\n",
      "Elapsed 73.5897 s, Epoch 5,  Iteration 100, loss = 0.1986\n",
      "Checking accuracy on validation set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "\n",
      "Elapsed 75.4442 s, Epoch 5,  Iteration 200, loss = 0.1983\n",
      "Checking accuracy on validation set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "\n",
      "Elapsed 77.4141 s, Epoch 5,  Iteration 300, loss = 0.2628\n",
      "Checking accuracy on validation set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "\n",
      "Elapsed 79.2287 s, Epoch 5,  Iteration 400, loss = 0.2402\n",
      "Checking accuracy on validation set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "\n",
      "Elapsed 81.0374 s, Epoch 5,  Iteration 500, loss = 0.2048\n",
      "Checking accuracy on validation set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "\n",
      "Elapsed 82.8432 s, Epoch 5,  Iteration 600, loss = 0.2271\n",
      "Checking accuracy on validation set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "\n",
      "Elapsed 84.6339 s, Epoch 5,  Iteration 700, loss = 0.4731\n",
      "Checking accuracy on validation set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "\n",
      "Elapsed 85.9491 s, Epoch 6,  Iteration 0, loss = 0.2274\n",
      "Checking accuracy on validation set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "\n",
      "Elapsed 88.0357 s, Epoch 6,  Iteration 100, loss = 0.5470\n",
      "Checking accuracy on validation set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "\n",
      "Elapsed 89.9882 s, Epoch 6,  Iteration 200, loss = 0.2979\n",
      "Checking accuracy on validation set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "\n",
      "Elapsed 91.7929 s, Epoch 6,  Iteration 300, loss = 0.1641\n",
      "Checking accuracy on validation set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "\n",
      "Elapsed 93.6022 s, Epoch 6,  Iteration 400, loss = 0.2513\n",
      "Checking accuracy on validation set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "\n",
      "Elapsed 95.3958 s, Epoch 6,  Iteration 500, loss = 0.1856\n",
      "Checking accuracy on validation set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "\n",
      "Elapsed 97.1902 s, Epoch 6,  Iteration 600, loss = 0.2222\n",
      "Checking accuracy on validation set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "\n",
      "Elapsed 99.0118 s, Epoch 6,  Iteration 700, loss = 0.2024\n",
      "Checking accuracy on validation set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "\n",
      "Elapsed 100.2814 s, Epoch 7,  Iteration 0, loss = 0.2540\n",
      "Checking accuracy on validation set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "\n",
      "Elapsed 102.1227 s, Epoch 7,  Iteration 100, loss = 0.2028\n",
      "Checking accuracy on validation set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "\n",
      "Elapsed 103.9906 s, Epoch 7,  Iteration 200, loss = 0.2396\n",
      "Checking accuracy on validation set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "\n",
      "Elapsed 105.8565 s, Epoch 7,  Iteration 300, loss = 0.2128\n",
      "Checking accuracy on validation set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "\n",
      "Elapsed 107.6819 s, Epoch 7,  Iteration 400, loss = 0.1771\n",
      "Checking accuracy on validation set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "\n",
      "Elapsed 109.4935 s, Epoch 7,  Iteration 500, loss = 0.1937\n",
      "Checking accuracy on validation set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "\n",
      "Elapsed 111.3069 s, Epoch 7,  Iteration 600, loss = 0.2912\n",
      "Checking accuracy on validation set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "\n",
      "Elapsed 113.1413 s, Epoch 7,  Iteration 700, loss = 0.1774\n",
      "Checking accuracy on validation set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "\n",
      "Elapsed 114.4130 s, Epoch 8,  Iteration 0, loss = 0.1576\n",
      "Checking accuracy on validation set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "\n",
      "Elapsed 116.2288 s, Epoch 8,  Iteration 100, loss = 0.2139\n",
      "Checking accuracy on validation set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 699 / 1000 correct (69.90)\n",
      "\n",
      "Elapsed 118.0510 s, Epoch 8,  Iteration 200, loss = 0.2458\n",
      "Checking accuracy on validation set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "\n",
      "Elapsed 119.9271 s, Epoch 8,  Iteration 300, loss = 0.2057\n",
      "Checking accuracy on validation set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "\n",
      "Elapsed 121.7659 s, Epoch 8,  Iteration 400, loss = 0.2667\n",
      "Checking accuracy on validation set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "\n",
      "Elapsed 123.9149 s, Epoch 8,  Iteration 500, loss = 0.1518\n",
      "Checking accuracy on validation set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "\n",
      "Elapsed 125.7615 s, Epoch 8,  Iteration 600, loss = 0.5332\n",
      "Checking accuracy on validation set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "\n",
      "Elapsed 127.5906 s, Epoch 8,  Iteration 700, loss = 0.3956\n",
      "Checking accuracy on validation set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "\n",
      "Elapsed 128.8820 s, Epoch 9,  Iteration 0, loss = 0.1421\n",
      "Checking accuracy on validation set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "\n",
      "Elapsed 130.7365 s, Epoch 9,  Iteration 100, loss = 0.2283\n",
      "Checking accuracy on validation set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "\n",
      "Elapsed 132.6183 s, Epoch 9,  Iteration 200, loss = 0.2339\n",
      "Checking accuracy on validation set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "\n",
      "Elapsed 134.4552 s, Epoch 9,  Iteration 300, loss = 0.2541\n",
      "Checking accuracy on validation set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "\n",
      "Elapsed 136.2572 s, Epoch 9,  Iteration 400, loss = 0.2375\n",
      "Checking accuracy on validation set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "\n",
      "Elapsed 138.0631 s, Epoch 9,  Iteration 500, loss = 0.2634\n",
      "Checking accuracy on validation set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "\n",
      "Elapsed 139.8766 s, Epoch 9,  Iteration 600, loss = 0.2841\n",
      "Checking accuracy on validation set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "\n",
      "Elapsed 141.6748 s, Epoch 9,  Iteration 700, loss = 0.2735\n",
      "Checking accuracy on validation set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train this model\n",
    "learning_rate = 2e-6\n",
    "\n",
    "optimizer = optim.Adam(params=fix_model.parameters(), lr=learning_rate)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9, nesterov=True)\n",
    "train_part34(fix_model, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Fixed Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 6929 / 10000 correct (69.29)\n",
      "\n",
      "Direct Fixed Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 6604 / 10000 correct (66.04)\n",
      "\n",
      "Original Floating Point Accuracy:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-77af271c04c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheck_accuracy_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect_fix_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nOriginal Floating Point Accuracy:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcheck_accuracy_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Finetune Fixed Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, fix_model)\n",
    "print(\"\\nDirect Fixed Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, direct_fix_model)\n",
    "print(\"\\nOriginal Floating Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next step: \n",
    "#1. implement the weight fixed \n",
    "#2. use the code provide by playground in the forward function\n",
    "#3. implement the convfixed layer, and fc fixed layer, bn fixed layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Set up the layers you need for a three-layer ConvNet with the  #\n",
    "        # architecture defined above.                                          #\n",
    "        ########################################################################\n",
    "        self.bits = 8\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, stride=1, padding=2)\n",
    "        #nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        #nn.init.constant_(self.conv1.bias, 0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        #nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        #nn.init.constant_(self.conv2.bias, 0)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        #self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.fc1 = nn.Linear(64*8*8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        #nn.init.kaiming_normal_(self.fc.weight)\n",
    "        self.linear_quantize = linear_quantization()\n",
    "        ########################################################################\n",
    "        #                          END OF YOUR CODE                            #       \n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward function for a 3-layer ConvNet. you      #\n",
    "        # should use the layers you defined in __init__ and specify the        #\n",
    "        # connectivity of those layers in forward()                            #\n",
    "        ########################################################################\n",
    "        x = self.linear_quantize.apply(x, 6)\n",
    "        \n",
    "        #self.conv1.weight = torch.nn.Parameter(linear_quantize(self.conv1.weight, 8))\n",
    "        #self.conv1.bias = torch.nn.Parameter(linear_quantize(self.conv1.bias, 8))\n",
    "        x = self.conv1(x)\n",
    "        #x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = self.linear_quantize.apply(x, 6)\n",
    "        #self.conv2.weight = torch.nn.Parameter(linear_quantize(self.conv2.weight, 8))\n",
    "        #self.conv2.bias = torch.nn.Parameter(linear_quantize(self.conv2.bias, 8))\n",
    "        x = self.conv2(x)\n",
    "        #x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = flatten(x)\n",
    "        \n",
    "        x = self.linear_quantize.apply(x, 6)\n",
    "        #self.fc1.weight = torch.nn.Parameter(linear_quantize(self.fc1.weight, 8))\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear_quantize.apply(x, 6)\n",
    "        #self.fc2.weight = torch.nn.Parameter(linear_quantize(self.fc2.weight, 8))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        scores = x\n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: 1. Implement different function\n",
    "#      2. differentiate FC and CONV\n",
    "def quantize_weight(model, bits):\n",
    "    for k, v in model.module.state_dict().items():\n",
    "        model.module.state_dict()[k] = linear_quantize(v, bits)\n",
    "    return model\n",
    "\n",
    "#TODO: Add a dictionary for bit width and function.\n",
    "def train_fixed_weight(model, optimizer, epochs=1, bits=8):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    t_begin = time.time()\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Add quantization for weight\n",
    "            model = quantize_weight(model, bits)\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                t_elapse = time.time() - t_begin\n",
    "                print('Elapsed %.4f s, Epoch %d,  Iteration %d, loss = %.4f' % (t_elapse, e, t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): FixedConvNet(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load module to loadmodel\n",
    "PATH = 'training.pt'\n",
    "\n",
    "model = ExpConvNet()\n",
    "fix_model = FixedConvNet()\n",
    "direct_fix_model = FixedConvNet()\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "fix_model.load_state_dict(torch.load(PATH))\n",
    "direct_fix_model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#Solve the weight type problem, change to cudafloat tensor\n",
    "if USE_GPU:\n",
    "    fix_model.cuda()\n",
    "    fix_model = torch.nn.DataParallel(fix_model, device_ids=range(torch.cuda.device_count()))\n",
    "\n",
    "if USE_GPU:\n",
    "    direct_fix_model.cuda()\n",
    "    direct_fix_model = torch.nn.DataParallel(direct_fix_model, device_ids=range(torch.cuda.device_count()))\n",
    "    #cudnn.benchmark = True\n",
    "\n",
    "if USE_GPU:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(direct_fix_model, device_ids=range(torch.cuda.device_count()))\n",
    "\n",
    "quantize_weight(fix_model, 8)\n",
    "quantize_weight(direct_fix_model, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 0.0291 s, Epoch 0,  Iteration 0, loss = 1.4879\n",
      "Checking accuracy on validation set\n",
      "Got 630 / 1000 correct (63.00)\n",
      "\n",
      "Elapsed 2.9690 s, Epoch 0,  Iteration 100, loss = 1.6069\n",
      "Checking accuracy on validation set\n",
      "Got 637 / 1000 correct (63.70)\n",
      "\n",
      "Elapsed 5.9893 s, Epoch 0,  Iteration 200, loss = 1.5094\n",
      "Checking accuracy on validation set\n",
      "Got 631 / 1000 correct (63.10)\n",
      "\n",
      "Elapsed 9.0018 s, Epoch 0,  Iteration 300, loss = 1.4684\n",
      "Checking accuracy on validation set\n",
      "Got 631 / 1000 correct (63.10)\n",
      "\n",
      "Elapsed 12.0180 s, Epoch 0,  Iteration 400, loss = 1.0851\n",
      "Checking accuracy on validation set\n",
      "Got 628 / 1000 correct (62.80)\n",
      "\n",
      "Elapsed 15.0667 s, Epoch 0,  Iteration 500, loss = 1.5274\n",
      "Checking accuracy on validation set\n",
      "Got 631 / 1000 correct (63.10)\n",
      "\n",
      "Elapsed 18.0918 s, Epoch 0,  Iteration 600, loss = 1.5095\n",
      "Checking accuracy on validation set\n",
      "Got 628 / 1000 correct (62.80)\n",
      "\n",
      "Elapsed 21.0738 s, Epoch 0,  Iteration 700, loss = 1.6062\n",
      "Checking accuracy on validation set\n",
      "Got 637 / 1000 correct (63.70)\n",
      "\n",
      "Elapsed 23.1057 s, Epoch 1,  Iteration 0, loss = 1.5266\n",
      "Checking accuracy on validation set\n",
      "Got 622 / 1000 correct (62.20)\n",
      "\n",
      "Elapsed 26.0518 s, Epoch 1,  Iteration 100, loss = 1.5975\n",
      "Checking accuracy on validation set\n",
      "Got 632 / 1000 correct (63.20)\n",
      "\n",
      "Elapsed 28.9991 s, Epoch 1,  Iteration 200, loss = 1.4900\n",
      "Checking accuracy on validation set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "\n",
      "Elapsed 31.9524 s, Epoch 1,  Iteration 300, loss = 0.9696\n",
      "Checking accuracy on validation set\n",
      "Got 628 / 1000 correct (62.80)\n",
      "\n",
      "Elapsed 34.9490 s, Epoch 1,  Iteration 400, loss = 1.5059\n",
      "Checking accuracy on validation set\n",
      "Got 633 / 1000 correct (63.30)\n",
      "\n",
      "Elapsed 37.9025 s, Epoch 1,  Iteration 500, loss = 1.5178\n",
      "Checking accuracy on validation set\n",
      "Got 628 / 1000 correct (62.80)\n",
      "\n",
      "Elapsed 40.8385 s, Epoch 1,  Iteration 600, loss = 1.5670\n",
      "Checking accuracy on validation set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "\n",
      "Elapsed 43.7692 s, Epoch 1,  Iteration 700, loss = 1.4536\n",
      "Checking accuracy on validation set\n",
      "Got 633 / 1000 correct (63.30)\n",
      "\n",
      "Elapsed 45.7944 s, Epoch 2,  Iteration 0, loss = 1.5505\n",
      "Checking accuracy on validation set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "\n",
      "Elapsed 48.7406 s, Epoch 2,  Iteration 100, loss = 1.5542\n",
      "Checking accuracy on validation set\n",
      "Got 637 / 1000 correct (63.70)\n",
      "\n",
      "Elapsed 51.6803 s, Epoch 2,  Iteration 200, loss = 1.5492\n",
      "Checking accuracy on validation set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "\n",
      "Elapsed 54.6392 s, Epoch 2,  Iteration 300, loss = 1.5727\n",
      "Checking accuracy on validation set\n",
      "Got 627 / 1000 correct (62.70)\n",
      "\n",
      "Elapsed 57.6621 s, Epoch 2,  Iteration 400, loss = 1.5876\n",
      "Checking accuracy on validation set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "\n",
      "Elapsed 60.7070 s, Epoch 2,  Iteration 500, loss = 1.5093\n",
      "Checking accuracy on validation set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "\n",
      "Elapsed 63.7247 s, Epoch 2,  Iteration 600, loss = 1.5772\n",
      "Checking accuracy on validation set\n",
      "Got 637 / 1000 correct (63.70)\n",
      "\n",
      "Elapsed 66.7379 s, Epoch 2,  Iteration 700, loss = 1.5817\n",
      "Checking accuracy on validation set\n",
      "Got 644 / 1000 correct (64.40)\n",
      "\n",
      "Elapsed 68.7862 s, Epoch 3,  Iteration 0, loss = 1.4650\n",
      "Checking accuracy on validation set\n",
      "Got 628 / 1000 correct (62.80)\n",
      "\n",
      "Elapsed 71.7237 s, Epoch 3,  Iteration 100, loss = 1.5671\n",
      "Checking accuracy on validation set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "\n",
      "Elapsed 74.6989 s, Epoch 3,  Iteration 200, loss = 1.4412\n",
      "Checking accuracy on validation set\n",
      "Got 631 / 1000 correct (63.10)\n",
      "\n",
      "Elapsed 77.7159 s, Epoch 3,  Iteration 300, loss = 1.5521\n",
      "Checking accuracy on validation set\n",
      "Got 627 / 1000 correct (62.70)\n",
      "\n",
      "Elapsed 80.6638 s, Epoch 3,  Iteration 400, loss = 1.5460\n",
      "Checking accuracy on validation set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "\n",
      "Elapsed 83.5952 s, Epoch 3,  Iteration 500, loss = 1.5751\n",
      "Checking accuracy on validation set\n",
      "Got 627 / 1000 correct (62.70)\n",
      "\n",
      "Elapsed 86.5332 s, Epoch 3,  Iteration 600, loss = 1.5313\n",
      "Checking accuracy on validation set\n",
      "Got 635 / 1000 correct (63.50)\n",
      "\n",
      "Elapsed 89.4647 s, Epoch 3,  Iteration 700, loss = 1.5186\n",
      "Checking accuracy on validation set\n",
      "Got 628 / 1000 correct (62.80)\n",
      "\n",
      "Elapsed 91.5175 s, Epoch 4,  Iteration 0, loss = 1.5688\n",
      "Checking accuracy on validation set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "\n",
      "Elapsed 94.5361 s, Epoch 4,  Iteration 100, loss = 1.5382\n",
      "Checking accuracy on validation set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "\n",
      "Elapsed 97.5569 s, Epoch 4,  Iteration 200, loss = 1.5289\n",
      "Checking accuracy on validation set\n",
      "Got 643 / 1000 correct (64.30)\n",
      "\n",
      "Elapsed 100.6102 s, Epoch 4,  Iteration 300, loss = 1.4813\n",
      "Checking accuracy on validation set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-4a42cb40a9a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#optimizer = optim.Adam(params=fix_model.parameters(), lr=learning_rate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfix_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_fixed_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfix_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-88f2519b19f8>\u001b[0m in \u001b[0;36mtrain_fixed_weight\u001b[0;34m(model, optimizer, epochs, bits)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# put model to training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# move to device, e.g. GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train this model\n",
    "learning_rate = 1e-6\n",
    "\n",
    "#optimizer = optim.Adam(params=fix_model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.SGD(fix_model.parameters(), lr=learning_rate,momentum=0.9, nesterov=True)\n",
    "train_fixed_weight(fix_model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Fixed Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 7011 / 10000 correct (70.11)\n",
      "\n",
      "Direct Fixed Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 6867 / 10000 correct (68.67)\n",
      "\n",
      "Original Floating Point Accuracy:\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-77af271c04c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheck_accuracy_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect_fix_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nOriginal Floating Point Accuracy:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcheck_accuracy_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-03301ce1ccb3>\u001b[0m in \u001b[0;36mcheck_accuracy_part34\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# move to device, e.g. GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-d6c0c5a14360>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# connectivity of those layers in forward()                            #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "print(\"Finetune Fixed Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, fix_model)\n",
    "print(\"\\nDirect Fixed Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, direct_fix_model)\n",
    "print(\"\\nOriginal Floating Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-198.2814,  255.2448,  117.6480, -207.7786, -189.1101],\n",
      "          [-103.1326,  251.6528,  -79.1741, -319.9017,  -12.8926],\n",
      "          [ 235.1852,   71.4552,  -19.8977, -286.5714,  216.7204],\n",
      "          [ 133.1982,  -64.3097, -264.7648,  126.2105,  -41.5371],\n",
      "          [  98.8290, -175.6736,  142.7939,  146.3576,  -59.8986]],\n",
      "\n",
      "         [[-251.8881, -104.8649, -257.0427,   79.2322,  175.4056],\n",
      "          [ -75.6155,   30.4536,  -73.4142,   42.4973,  243.2123],\n",
      "          [ 227.3108, -119.0568,   55.2801,   95.9392,  -11.9220],\n",
      "          [ 261.6974,   89.3977, -313.3453,  363.6068,  150.8462],\n",
      "          [ 256.3130, -276.7975, -355.8281,  401.6050,   18.7703]],\n",
      "\n",
      "         [[  46.9333,  237.2028,  129.9361,   20.4777,   22.2818],\n",
      "          [-229.5939,  169.2838,   20.6609,  111.7582,    4.7855],\n",
      "          [-165.5342,  -91.6710,   -9.8183,  -51.4141,  -62.1809],\n",
      "          [ 337.9961,  -96.6034, -162.7282,  -78.8396,   31.5714],\n",
      "          [  37.1734,   -9.6663, -127.2759, -253.4323, -148.9542]]],\n",
      "\n",
      "\n",
      "        [[[ 200.0011,  178.1546,  197.8615, -181.4164,  199.8802],\n",
      "          [ -80.7904,   -7.6254, -223.0728,  193.1431, -131.4914],\n",
      "          [ 120.2254, -161.8636, -104.9749,   34.5291,    3.0421],\n",
      "          [ -84.5481,   43.2944,  -60.8316, -177.8413, -133.3268],\n",
      "          [-234.0753, -250.2168,    0.4233, -205.7664, -198.2386]],\n",
      "\n",
      "         [[ 197.8664,  -85.3288,   88.8419,   -7.1772,  122.6109],\n",
      "          [ 187.1294,  212.3609,   46.6272,  197.5644,   27.8813],\n",
      "          [ -74.6110,  -50.5891,  -44.4429,   -8.8945,  248.0839],\n",
      "          [ 179.4999, -182.7308,  -94.7141, -289.2698, -122.5806],\n",
      "          [  72.4739,    3.0045, -163.3427,   16.5499, -176.3210]],\n",
      "\n",
      "         [[-186.9752,  -41.9504,  287.4312,   17.9488,  -97.8879],\n",
      "          [  15.3889,  136.5512,  -21.2025, -182.0746,  -12.4092],\n",
      "          [-118.4051,  -24.5181,  185.7738,  204.8595, -140.1131],\n",
      "          [-201.2928,  218.2913,  323.8619,   87.9350,  -63.2505],\n",
      "          [   9.7876,  152.2811,   56.2970,  142.8787, -182.1942]]],\n",
      "\n",
      "\n",
      "        [[[ 127.6880, -166.3058,  111.8529,    5.9529,   -9.7691],\n",
      "          [  51.2283,  -61.8856,  -91.1051,    3.3456,  -13.7131],\n",
      "          [ 211.1655,  -52.2234,   44.1763,  125.6279,   39.0781],\n",
      "          [-336.7621,   51.2611,   14.7283,  331.8279, -236.3543],\n",
      "          [-117.2514,  143.3544, -158.8422,  -69.3448,  -53.5928]],\n",
      "\n",
      "         [[  54.9081,   -8.2544,  213.2658, -176.9472,  -94.6802],\n",
      "          [  98.8138,  117.6340, -145.9907,   31.9177,  270.2360],\n",
      "          [ -60.2433, -344.0127,  225.4430,  -73.2903,  -71.7704],\n",
      "          [-182.8486,  210.1451,   79.4522,  -99.5350, -285.4413],\n",
      "          [ -18.9431,  181.1265,   14.8307,  309.0965,   19.7573]],\n",
      "\n",
      "         [[  69.6823,  162.8738, -247.8393,   -2.1021, -121.6483],\n",
      "          [  16.8810, -263.5927, -378.9791, -172.9823,  208.6763],\n",
      "          [-169.5744, -167.7555,  195.3461,  544.6024,  197.5276],\n",
      "          [  -0.0747, -184.6879,  204.4107,  377.2695, -626.4212],\n",
      "          [-125.8693,  403.3427, -185.3322, -148.5173,  123.6597]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -60.1116,  -45.9625,   25.2653,  127.1687,  -13.2040],\n",
      "          [ 274.6117,  -16.7152,  -68.2330,  229.0948, -101.0548],\n",
      "          [ -55.4914,  242.7155,  154.1629,   -7.1092, -311.1519],\n",
      "          [-203.2455,    7.6553,  103.3045, -102.5223,  -13.1743],\n",
      "          [  24.3952,  219.3768,  291.4542, -575.0469,   57.5187]],\n",
      "\n",
      "         [[ 103.5446, -236.9959, -414.9496,  -31.8814,   59.6859],\n",
      "          [ 106.9593,  206.9286, -213.5536, -241.2837, -295.5966],\n",
      "          [ -82.3217,  103.7436,  169.4003,  -35.1727, -175.2439],\n",
      "          [  86.7115, -264.4635,  261.9442,   65.5402,   49.7970],\n",
      "          [ -62.4245,   13.0723,  235.2114,   66.1526,  231.7780]],\n",
      "\n",
      "         [[ 192.5788,  -18.7022, -165.1956,  -53.5885, -273.5742],\n",
      "          [-128.0382,  333.2508,  104.1888, -201.0034, -312.9747],\n",
      "          [  33.0576,  300.1584,  256.5858,  116.3538, -205.6156],\n",
      "          [-252.3382,  273.3961,   37.3994,  187.5786,   -2.2959],\n",
      "          [-295.7195, -125.6107,   84.7048,   15.5678,  155.3142]]],\n",
      "\n",
      "\n",
      "        [[[-180.2353,  216.6201, -219.7980,   -1.4402,   33.5088],\n",
      "          [ -19.9098, -162.6290,   99.6278,   62.7488, -176.3773],\n",
      "          [-102.5619,   48.3477,  -91.8632, -145.4081,  173.2540],\n",
      "          [   7.5542,   89.0751,  -13.0288,  -46.3351,  245.1204],\n",
      "          [ -57.3161,   88.9759,  136.9827, -364.2197,   23.1624]],\n",
      "\n",
      "         [[-177.3201, -117.7773,  320.3268,  135.6563,  114.6327],\n",
      "          [   2.0521,   66.0593,   85.4411,   99.6092,  158.1460],\n",
      "          [-142.8582,   -6.5567,   -0.2192, -146.9066,  342.7377],\n",
      "          [-104.0831,   22.6219, -202.4072, -112.9146,    2.9820],\n",
      "          [  -2.3650,  -27.6454, -173.4029,   79.4413, -131.7020]],\n",
      "\n",
      "         [[  -5.1178, -231.5542,   39.4483,  102.4851,   35.0907],\n",
      "          [-278.5711, -141.4806,  258.4470, -222.2273,    2.5049],\n",
      "          [ 103.0411, -257.9835,    4.4211,  293.4292,  306.2906],\n",
      "          [  -0.9418, -151.8677,   47.8776, -157.4377,  336.6356],\n",
      "          [ -61.4115,  -94.1809,   50.9843,   17.9062,  198.2281]]],\n",
      "\n",
      "\n",
      "        [[[-264.8149,  -12.5525,  -18.1471,   33.2687, -238.6851],\n",
      "          [ 155.0648,  -33.4528,   50.4867,  184.2129,   36.3585],\n",
      "          [ -64.1029, -227.1499,    1.1901, -150.7746,  -82.5157],\n",
      "          [ 540.8966, -126.2673, -149.1435, -142.0134,   62.5688],\n",
      "          [ -41.9019,  190.6466,   -1.9666,  218.7408,  136.9765]],\n",
      "\n",
      "         [[  26.0985,  -33.0662,  581.6558,  181.1683, -114.8004],\n",
      "          [-475.5923,  311.4924,   32.8967,  146.4288,   85.3521],\n",
      "          [ -70.5550, -169.3291,  -56.5608,    2.4743,  111.6928],\n",
      "          [  28.6486, -125.6425,  145.5541, -186.4872, -409.4834],\n",
      "          [-114.7094,  131.0113, -345.2665,  -30.5687,   58.4738]],\n",
      "\n",
      "         [[ -94.2657, -128.6212,  300.4352, -262.8008, -185.8249],\n",
      "          [-273.4633,   99.4216,  424.5159,  -44.7913,   55.3303],\n",
      "          [ 211.6395,   61.2101,   42.2692, -197.0287,   20.9500],\n",
      "          [ 108.1189,  285.1844,  -97.6211, -163.8457,  -90.1795],\n",
      "          [ -57.9451,  115.0400,  -23.7941,  108.8186,  -21.9791]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(direct_fix_model.module.state_dict()['conv1.weight']*1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
