{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "#from utee import misc, quant, selector\n",
    "\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Load CIFAR-10\n",
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "def test_flatten():\n",
    "    x = torch.arange(12).view(2, 1, 3, 2)\n",
    "    print('Before flattening: ', x)\n",
    "    print('After flattening: ', flatten(x))\n",
    "\n",
    "#test_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    t_begin = time.time()\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            if t % print_every == 0:\n",
    "                t_elapse = time.time() - t_begin\n",
    "                print('Elapsed %.4f s, Epoch %d,  Iteration %d, loss = %.4f' % (t_elapse, e, t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #         \n",
    "# Experiment with any architectures, optimizers, and hyperparameters.          #\n",
    "# Achieve AT LEAST 70% accuracy on the *validation set* within 10 epochs.      #\n",
    "#                                                                              #\n",
    "# Note that you can use the check_accuracy function to evaluate on either      #\n",
    "# the test set or the validation set, by passing either loader_test or         #\n",
    "# loader_val as the second argument to check_accuracy. You should not touch    #\n",
    "# the test set until you have finished your architecture and  hyperparameter   #\n",
    "# tuning, and only run the test set once at the end to report a final value.   #\n",
    "################################################################################\n",
    "\n",
    "'''\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 5,stride=1, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "    nn.Conv2d(32, 64, 3,stride=1,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "    Flatten(),\n",
    "    nn.Linear(4096,512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512,10),\n",
    ")\n",
    "'''\n",
    "\n",
    "class ExpConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Set up the layers you need for a three-layer ConvNet with the  #\n",
    "        # architecture defined above.                                          #\n",
    "        ########################################################################\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, stride=1, padding=2)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.constant_(self.conv1.bias, 0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.constant_(self.conv2.bias, 0)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64*8*8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "        ########################################################################\n",
    "        #                          END OF YOUR CODE                            #       \n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward function for a 3-layer ConvNet. you      #\n",
    "        # should use the layers you defined in __init__ and specify the        #\n",
    "        # connectivity of those layers in forward()                            #\n",
    "        ########################################################################\n",
    "        x = self.conv1(x)\n",
    "        #x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = self.conv2(x)\n",
    "        #x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        scores = x\n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 0.0134 s, Epoch 0,  Iteration 0, loss = 5.7583\n",
      "Checking accuracy on validation set\n",
      "Got 113 / 1000 correct (11.30)\n",
      "\n",
      "Elapsed 1.3902 s, Epoch 0,  Iteration 100, loss = 1.8266\n",
      "Checking accuracy on validation set\n",
      "Got 383 / 1000 correct (38.30)\n",
      "\n",
      "Elapsed 2.7424 s, Epoch 0,  Iteration 200, loss = 1.4599\n",
      "Checking accuracy on validation set\n",
      "Got 482 / 1000 correct (48.20)\n",
      "\n",
      "Elapsed 4.1081 s, Epoch 0,  Iteration 300, loss = 1.3743\n",
      "Checking accuracy on validation set\n",
      "Got 503 / 1000 correct (50.30)\n",
      "\n",
      "Elapsed 5.4676 s, Epoch 0,  Iteration 400, loss = 1.1487\n",
      "Checking accuracy on validation set\n",
      "Got 525 / 1000 correct (52.50)\n",
      "\n",
      "Elapsed 6.8207 s, Epoch 0,  Iteration 500, loss = 1.3694\n",
      "Checking accuracy on validation set\n",
      "Got 558 / 1000 correct (55.80)\n",
      "\n",
      "Elapsed 8.1550 s, Epoch 0,  Iteration 600, loss = 1.2151\n",
      "Checking accuracy on validation set\n",
      "Got 600 / 1000 correct (60.00)\n",
      "\n",
      "Elapsed 9.4865 s, Epoch 0,  Iteration 700, loss = 1.1405\n",
      "Checking accuracy on validation set\n",
      "Got 587 / 1000 correct (58.70)\n",
      "\n",
      "Elapsed 10.4294 s, Epoch 1,  Iteration 0, loss = 1.1080\n",
      "Checking accuracy on validation set\n",
      "Got 568 / 1000 correct (56.80)\n",
      "\n",
      "Elapsed 11.7644 s, Epoch 1,  Iteration 100, loss = 0.8738\n",
      "Checking accuracy on validation set\n",
      "Got 600 / 1000 correct (60.00)\n",
      "\n",
      "Elapsed 13.1055 s, Epoch 1,  Iteration 200, loss = 1.1607\n",
      "Checking accuracy on validation set\n",
      "Got 622 / 1000 correct (62.20)\n",
      "\n",
      "Elapsed 14.4768 s, Epoch 1,  Iteration 300, loss = 0.7409\n",
      "Checking accuracy on validation set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "\n",
      "Elapsed 15.8468 s, Epoch 1,  Iteration 400, loss = 1.1155\n",
      "Checking accuracy on validation set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "\n",
      "Elapsed 17.2066 s, Epoch 1,  Iteration 500, loss = 1.0096\n",
      "Checking accuracy on validation set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "\n",
      "Elapsed 18.5638 s, Epoch 1,  Iteration 600, loss = 1.1035\n",
      "Checking accuracy on validation set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "\n",
      "Elapsed 19.9005 s, Epoch 1,  Iteration 700, loss = 1.0508\n",
      "Checking accuracy on validation set\n",
      "Got 643 / 1000 correct (64.30)\n",
      "\n",
      "Elapsed 20.8567 s, Epoch 2,  Iteration 0, loss = 0.7822\n",
      "Checking accuracy on validation set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "\n",
      "Elapsed 22.1809 s, Epoch 2,  Iteration 100, loss = 0.6497\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 23.5080 s, Epoch 2,  Iteration 200, loss = 0.9212\n",
      "Checking accuracy on validation set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "\n",
      "Elapsed 24.8365 s, Epoch 2,  Iteration 300, loss = 0.9281\n",
      "Checking accuracy on validation set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "\n",
      "Elapsed 26.2060 s, Epoch 2,  Iteration 400, loss = 0.7259\n",
      "Checking accuracy on validation set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "\n",
      "Elapsed 27.5814 s, Epoch 2,  Iteration 500, loss = 0.6623\n",
      "Checking accuracy on validation set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "\n",
      "Elapsed 28.9331 s, Epoch 2,  Iteration 600, loss = 0.6667\n",
      "Checking accuracy on validation set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "\n",
      "Elapsed 30.2502 s, Epoch 2,  Iteration 700, loss = 0.7600\n",
      "Checking accuracy on validation set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "\n",
      "Elapsed 31.1834 s, Epoch 3,  Iteration 0, loss = 0.4814\n",
      "Checking accuracy on validation set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "\n",
      "Elapsed 32.4894 s, Epoch 3,  Iteration 100, loss = 0.5105\n",
      "Checking accuracy on validation set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "\n",
      "Elapsed 33.8020 s, Epoch 3,  Iteration 200, loss = 0.5426\n",
      "Checking accuracy on validation set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "\n",
      "Elapsed 35.1225 s, Epoch 3,  Iteration 300, loss = 0.4687\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 36.4331 s, Epoch 3,  Iteration 400, loss = 0.5709\n",
      "Checking accuracy on validation set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "\n",
      "Elapsed 37.7458 s, Epoch 3,  Iteration 500, loss = 0.6911\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 39.0580 s, Epoch 3,  Iteration 600, loss = 0.7126\n",
      "Checking accuracy on validation set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "\n",
      "Elapsed 40.3707 s, Epoch 3,  Iteration 700, loss = 0.5952\n",
      "Checking accuracy on validation set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "\n",
      "Elapsed 41.3203 s, Epoch 4,  Iteration 0, loss = 0.5207\n",
      "Checking accuracy on validation set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "\n",
      "Elapsed 42.6710 s, Epoch 4,  Iteration 100, loss = 0.3004\n",
      "Checking accuracy on validation set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "\n",
      "Elapsed 43.9992 s, Epoch 4,  Iteration 200, loss = 0.3533\n",
      "Checking accuracy on validation set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "\n",
      "Elapsed 45.3272 s, Epoch 4,  Iteration 300, loss = 0.3209\n",
      "Checking accuracy on validation set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "\n",
      "Elapsed 46.6750 s, Epoch 4,  Iteration 400, loss = 0.4620\n",
      "Checking accuracy on validation set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "\n",
      "Elapsed 48.0117 s, Epoch 4,  Iteration 500, loss = 0.3994\n",
      "Checking accuracy on validation set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "\n",
      "Elapsed 49.3495 s, Epoch 4,  Iteration 600, loss = 0.4582\n",
      "Checking accuracy on validation set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "\n",
      "Elapsed 50.6796 s, Epoch 4,  Iteration 700, loss = 0.3680\n",
      "Checking accuracy on validation set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ExpConvNet()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "# You should get at least 70% accuracy\n",
    "train_part34(model, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../pretrain_model/training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.quantization import *\n",
    "\n",
    "class FixedLayerConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Set up the layers you need for a three-layer ConvNet with the  #\n",
    "        # architecture defined above.                                          #\n",
    "        ########################################################################\n",
    "        self.bits = 8\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, stride=1, padding=2)\n",
    "        #nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        #nn.init.constant_(self.conv1.bias, 0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        #nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        #nn.init.constant_(self.conv2.bias, 0)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        #self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.fc1 = nn.Linear(64*8*8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        #nn.init.kaiming_normal_(self.fc.weight)\n",
    "        self.quant = activation_quantization(8, Quant.linear)\n",
    "        ########################################################################\n",
    "        #                          END OF YOUR CODE                            #       \n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward function for a 3-layer ConvNet. you      #\n",
    "        # should use the layers you defined in __init__ and specify the        #\n",
    "        # connectivity of those layers in forward()                            #\n",
    "        ########################################################################\n",
    "        #x = quantization.apply(x, self.bits, Quant.linear)\n",
    "        x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        #x = quantization.apply(x, self.bits, Quant.linear)\n",
    "        x = self.quant(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = flatten(x)\n",
    "        x = self.quant(x)\n",
    "        #x = quantization.apply(x, self.bits, Quant.linear)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.quant(x)\n",
    "        #x = quantization.apply(x, self.bits, Quant.linear)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        scores = x\n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################\n",
    "        return scores\n",
    "\n",
    "fix_model = FixedLayerConvNet()\n",
    "direct_fix_model = FixedLayerConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FixedLayerConvNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (quant): activation_quantization()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '../pretrain_model/training.pt'\n",
    "model_pretrain = torch.load(PATH)\n",
    "fix_model\n",
    "#model_pretrain\n",
    "#for k, v in model.module.state_dict().items():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load module to loadmodel\n",
    "PATH = '../pretrain_model/training.pt'\n",
    "fix_model.load_state_dict(torch.load(PATH))\n",
    "direct_fix_model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#Solve the weight type problem, change to cudafloat tensor\n",
    "if USE_GPU:\n",
    "    fix_model.cuda()\n",
    "    fix_model = torch.nn.DataParallel(fix_model, device_ids=range(torch.cuda.device_count()))\n",
    "\n",
    "if USE_GPU:\n",
    "    direct_fix_model.cuda()\n",
    "    direct_fix_model = torch.nn.DataParallel(direct_fix_model, device_ids=range(torch.cuda.device_count()))\n",
    "    #cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 0.0177 s, Epoch 0,  Iteration 0, loss = 0.3403\n",
      "Checking accuracy on validation set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "\n",
      "Elapsed 1.6932 s, Epoch 0,  Iteration 100, loss = 0.2090\n",
      "Checking accuracy on validation set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "\n",
      "Elapsed 3.3739 s, Epoch 0,  Iteration 200, loss = 0.3896\n",
      "Checking accuracy on validation set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "\n",
      "Elapsed 5.0427 s, Epoch 0,  Iteration 300, loss = 0.3283\n",
      "Checking accuracy on validation set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "\n",
      "Elapsed 6.7102 s, Epoch 0,  Iteration 400, loss = 0.2870\n",
      "Checking accuracy on validation set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "\n",
      "Elapsed 8.3894 s, Epoch 0,  Iteration 500, loss = 0.3506\n",
      "Checking accuracy on validation set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "\n",
      "Elapsed 10.0648 s, Epoch 0,  Iteration 600, loss = 0.2843\n",
      "Checking accuracy on validation set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "\n",
      "Elapsed 11.7585 s, Epoch 0,  Iteration 700, loss = 0.2298\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 12.9617 s, Epoch 1,  Iteration 0, loss = 0.2896\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 14.6383 s, Epoch 1,  Iteration 100, loss = 0.2105\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 16.3153 s, Epoch 1,  Iteration 200, loss = 0.2440\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 17.9960 s, Epoch 1,  Iteration 300, loss = 0.2333\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 19.6715 s, Epoch 1,  Iteration 400, loss = 0.2954\n",
      "Checking accuracy on validation set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "\n",
      "Elapsed 21.3500 s, Epoch 1,  Iteration 500, loss = 0.3279\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 23.0271 s, Epoch 1,  Iteration 600, loss = 0.3075\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 24.7103 s, Epoch 1,  Iteration 700, loss = 0.2324\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 25.8874 s, Epoch 2,  Iteration 0, loss = 0.2590\n",
      "Checking accuracy on validation set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "\n",
      "Elapsed 27.5710 s, Epoch 2,  Iteration 100, loss = 0.2966\n",
      "Checking accuracy on validation set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "\n",
      "Elapsed 29.2648 s, Epoch 2,  Iteration 200, loss = 0.2641\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 30.9344 s, Epoch 2,  Iteration 300, loss = 0.2624\n",
      "Checking accuracy on validation set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "\n",
      "Elapsed 32.6196 s, Epoch 2,  Iteration 400, loss = 0.2173\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 34.3206 s, Epoch 2,  Iteration 500, loss = 0.1803\n",
      "Checking accuracy on validation set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "\n",
      "Elapsed 36.0243 s, Epoch 2,  Iteration 600, loss = 0.2226\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 37.7095 s, Epoch 2,  Iteration 700, loss = 0.2131\n",
      "Checking accuracy on validation set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "\n",
      "Elapsed 38.8814 s, Epoch 3,  Iteration 0, loss = 0.3348\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 40.5667 s, Epoch 3,  Iteration 100, loss = 0.3509\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 42.2649 s, Epoch 3,  Iteration 200, loss = 0.2973\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 43.9527 s, Epoch 3,  Iteration 300, loss = 0.2822\n",
      "Checking accuracy on validation set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "\n",
      "Elapsed 45.6434 s, Epoch 3,  Iteration 400, loss = 0.1817\n",
      "Checking accuracy on validation set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "\n",
      "Elapsed 47.3363 s, Epoch 3,  Iteration 500, loss = 0.3144\n",
      "Checking accuracy on validation set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "\n",
      "Elapsed 49.0151 s, Epoch 3,  Iteration 600, loss = 0.3261\n",
      "Checking accuracy on validation set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "\n",
      "Elapsed 50.7037 s, Epoch 3,  Iteration 700, loss = 0.2173\n",
      "Checking accuracy on validation set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "\n",
      "Elapsed 51.8879 s, Epoch 4,  Iteration 0, loss = 0.2587\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 53.5757 s, Epoch 4,  Iteration 100, loss = 0.2846\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 55.2746 s, Epoch 4,  Iteration 200, loss = 0.2722\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 56.9680 s, Epoch 4,  Iteration 300, loss = 0.1927\n",
      "Checking accuracy on validation set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "\n",
      "Elapsed 58.6415 s, Epoch 4,  Iteration 400, loss = 0.2256\n",
      "Checking accuracy on validation set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "\n",
      "Elapsed 60.3293 s, Epoch 4,  Iteration 500, loss = 0.2863\n",
      "Checking accuracy on validation set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "\n",
      "Elapsed 62.0389 s, Epoch 4,  Iteration 600, loss = 0.2316\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 63.7145 s, Epoch 4,  Iteration 700, loss = 0.2345\n",
      "Checking accuracy on validation set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "\n",
      "Elapsed 64.9033 s, Epoch 5,  Iteration 0, loss = 0.3747\n",
      "Checking accuracy on validation set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "\n",
      "Elapsed 66.6017 s, Epoch 5,  Iteration 100, loss = 0.3284\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 68.3005 s, Epoch 5,  Iteration 200, loss = 0.3015\n",
      "Checking accuracy on validation set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "\n",
      "Elapsed 70.0015 s, Epoch 5,  Iteration 300, loss = 0.2488\n",
      "Checking accuracy on validation set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "\n",
      "Elapsed 71.6931 s, Epoch 5,  Iteration 400, loss = 0.2567\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 73.3848 s, Epoch 5,  Iteration 500, loss = 0.2849\n",
      "Checking accuracy on validation set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "\n",
      "Elapsed 75.0634 s, Epoch 5,  Iteration 600, loss = 0.2885\n",
      "Checking accuracy on validation set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "\n",
      "Elapsed 76.7416 s, Epoch 5,  Iteration 700, loss = 0.2901\n",
      "Checking accuracy on validation set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "\n",
      "Elapsed 77.9186 s, Epoch 6,  Iteration 0, loss = 0.1561\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 79.6265 s, Epoch 6,  Iteration 100, loss = 0.1532\n",
      "Checking accuracy on validation set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "\n",
      "Elapsed 81.3114 s, Epoch 6,  Iteration 200, loss = 0.3668\n",
      "Checking accuracy on validation set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "\n",
      "Elapsed 82.9999 s, Epoch 6,  Iteration 300, loss = 0.2380\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 84.6810 s, Epoch 6,  Iteration 400, loss = 0.1964\n",
      "Checking accuracy on validation set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "\n",
      "Elapsed 86.3716 s, Epoch 6,  Iteration 500, loss = 0.1780\n",
      "Checking accuracy on validation set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "\n",
      "Elapsed 88.0616 s, Epoch 6,  Iteration 600, loss = 0.1373\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 89.7656 s, Epoch 6,  Iteration 700, loss = 0.2774\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 90.9471 s, Epoch 7,  Iteration 0, loss = 0.2213\n",
      "Checking accuracy on validation set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "\n",
      "Elapsed 92.6384 s, Epoch 7,  Iteration 100, loss = 0.2817\n",
      "Checking accuracy on validation set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "\n",
      "Elapsed 94.3342 s, Epoch 7,  Iteration 200, loss = 0.2181\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 96.0314 s, Epoch 7,  Iteration 300, loss = 0.2427\n",
      "Checking accuracy on validation set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "\n",
      "Elapsed 97.7176 s, Epoch 7,  Iteration 400, loss = 0.3237\n",
      "Checking accuracy on validation set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "\n",
      "Elapsed 99.4000 s, Epoch 7,  Iteration 500, loss = 0.5353\n",
      "Checking accuracy on validation set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "\n",
      "Elapsed 101.0724 s, Epoch 7,  Iteration 600, loss = 0.1958\n",
      "Checking accuracy on validation set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "\n",
      "Elapsed 102.7756 s, Epoch 7,  Iteration 700, loss = 0.1355\n",
      "Checking accuracy on validation set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "\n",
      "Elapsed 103.9696 s, Epoch 8,  Iteration 0, loss = 0.1706\n",
      "Checking accuracy on validation set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "\n",
      "Elapsed 105.6761 s, Epoch 8,  Iteration 100, loss = 0.2213\n",
      "Checking accuracy on validation set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 689 / 1000 correct (68.90)\n",
      "\n",
      "Elapsed 107.3777 s, Epoch 8,  Iteration 200, loss = 0.2693\n",
      "Checking accuracy on validation set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "\n",
      "Elapsed 109.0759 s, Epoch 8,  Iteration 300, loss = 0.3329\n",
      "Checking accuracy on validation set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "\n",
      "Elapsed 110.7750 s, Epoch 8,  Iteration 400, loss = 0.2182\n",
      "Checking accuracy on validation set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "\n",
      "Elapsed 112.4724 s, Epoch 8,  Iteration 500, loss = 0.1640\n",
      "Checking accuracy on validation set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "\n",
      "Elapsed 114.1627 s, Epoch 8,  Iteration 600, loss = 0.3199\n",
      "Checking accuracy on validation set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "\n",
      "Elapsed 115.8596 s, Epoch 8,  Iteration 700, loss = 0.2301\n",
      "Checking accuracy on validation set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "\n",
      "Elapsed 117.0332 s, Epoch 9,  Iteration 0, loss = 0.2050\n",
      "Checking accuracy on validation set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "\n",
      "Elapsed 118.7322 s, Epoch 9,  Iteration 100, loss = 0.2296\n",
      "Checking accuracy on validation set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "\n",
      "Elapsed 120.4267 s, Epoch 9,  Iteration 200, loss = 0.4660\n",
      "Checking accuracy on validation set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "\n",
      "Elapsed 122.0967 s, Epoch 9,  Iteration 300, loss = 0.2233\n",
      "Checking accuracy on validation set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "\n",
      "Elapsed 123.7678 s, Epoch 9,  Iteration 400, loss = 0.5030\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 125.4590 s, Epoch 9,  Iteration 500, loss = 0.2748\n",
      "Checking accuracy on validation set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "\n",
      "Elapsed 127.1346 s, Epoch 9,  Iteration 600, loss = 0.3415\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 128.8093 s, Epoch 9,  Iteration 700, loss = 0.2318\n",
      "Checking accuracy on validation set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train this model\n",
    "learning_rate = 2e-6\n",
    "\n",
    "optimizer = optim.Adam(params=fix_model.parameters(), lr=learning_rate)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9, nesterov=True)\n",
    "train_part34(fix_model, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Fixed Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 6876 / 10000 correct (68.76)\n",
      "\n",
      "Direct Fixed Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 6812 / 10000 correct (68.12)\n",
      "\n",
      "Original Floating Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 6950 / 10000 correct (69.50)\n"
     ]
    }
   ],
   "source": [
    "print(\"Finetune Fixed Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, fix_model)\n",
    "print(\"\\nDirect Fixed Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, direct_fix_model)\n",
    "print(\"\\nOriginal Floating Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next step: \n",
    "#1. implement the weight fixed \n",
    "#2. use the code provide by playground in the forward function\n",
    "#3. implement the convfixed layer, and fc fixed layer, bn fixed layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Set up the layers you need for a three-layer ConvNet with the  #\n",
    "        # architecture defined above.                                          #\n",
    "        ########################################################################\n",
    "        self.bits = 8\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, stride=1, padding=2)\n",
    "        #nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        #nn.init.constant_(self.conv1.bias, 0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        #nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        #nn.init.constant_(self.conv2.bias, 0)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        #self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.fc1 = nn.Linear(64*8*8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        #nn.init.kaiming_normal_(self.fc.weight)\n",
    "        self.act_quant = activation_quantization(8, Quant.linear)\n",
    "        ########################################################################\n",
    "        #                          END OF YOUR CODE                            #       \n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward function for a 3-layer ConvNet. you      #\n",
    "        # should use the layers you defined in __init__ and specify the        #\n",
    "        # connectivity of those layers in forward()                            #\n",
    "        ########################################################################\n",
    "        x = self.act_quant(x)\n",
    "        \n",
    "        #self.conv1.weight = torch.nn.Parameter(linear_quantize(self.conv1.weight, 8))\n",
    "        #self.conv1.bias = torch.nn.Parameter(linear_quantize(self.conv1.bias, 8))\n",
    "        x = self.conv1(x)\n",
    "        #x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = self.act_quant(x)\n",
    "        #self.conv2.weight = torch.nn.Parameter(linear_quantize(self.conv2.weight, 8))\n",
    "        #self.conv2.bias = torch.nn.Parameter(linear_quantize(self.conv2.bias, 8))\n",
    "        x = self.conv2(x)\n",
    "        #x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(F.relu(x))\n",
    "        x = flatten(x)\n",
    "        \n",
    "        x = self.act_quant(x)\n",
    "        #self.fc1.weight = torch.nn.Parameter(linear_quantize(self.fc1.weight, 8))\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.act_quant(x)\n",
    "        #self.fc2.weight = torch.nn.Parameter(linear_quantize(self.fc2.weight, 8))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        scores = x\n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: 1. Implement different function\n",
    "#      2. differentiate FC and CONV\n",
    "def quantize_weight(model, bits):\n",
    "    for k, v in model.module.state_dict().items():\n",
    "        model.module.state_dict()[k] = Quant.linear(v, bits)\n",
    "    return model\n",
    "\n",
    "#TODO: Add a dictionary for bit width and function.\n",
    "def train_fixed_weight(model, optimizer, epochs=1, bits=8):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    t_begin = time.time()\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Add quantization for weight\n",
    "            model = quantize_weight(model, bits)\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                t_elapse = time.time() - t_begin\n",
    "                print('Elapsed %.4f s, Epoch %d,  Iteration %d, loss = %.4f' % (t_elapse, e, t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): FixedConvNet(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (act_quant): activation_quantization()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load module to loadmodel\n",
    "PATH = '../pretrain_model/training.pt'\n",
    "\n",
    "model = ExpConvNet()\n",
    "fix_model = FixedConvNet()\n",
    "direct_fix_model = FixedConvNet()\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "fix_model.load_state_dict(torch.load(PATH))\n",
    "direct_fix_model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#Solve the weight type problem, change to cudafloat tensor\n",
    "if USE_GPU:\n",
    "    fix_model.cuda()\n",
    "    fix_model = torch.nn.DataParallel(fix_model, device_ids=range(torch.cuda.device_count()))\n",
    "\n",
    "if USE_GPU:\n",
    "    direct_fix_model.cuda()\n",
    "    direct_fix_model = torch.nn.DataParallel(direct_fix_model, device_ids=range(torch.cuda.device_count()))\n",
    "    #cudnn.benchmark = True\n",
    "\n",
    "if USE_GPU:\n",
    "    model.cuda()\n",
    "    model = torch.nn.DataParallel(direct_fix_model, device_ids=range(torch.cuda.device_count()))\n",
    "\n",
    "quantize_weight(fix_model, 8)\n",
    "quantize_weight(direct_fix_model, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 0.0297 s, Epoch 0,  Iteration 0, loss = 0.2896\n",
      "Checking accuracy on validation set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "\n",
      "Elapsed 2.9527 s, Epoch 0,  Iteration 100, loss = 0.3342\n",
      "Checking accuracy on validation set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "\n",
      "Elapsed 5.8740 s, Epoch 0,  Iteration 200, loss = 0.2804\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 8.7894 s, Epoch 0,  Iteration 300, loss = 0.3075\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 11.7196 s, Epoch 0,  Iteration 400, loss = 0.2005\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 14.6372 s, Epoch 0,  Iteration 500, loss = 0.2506\n",
      "Checking accuracy on validation set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "\n",
      "Elapsed 17.5648 s, Epoch 0,  Iteration 600, loss = 0.2783\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 20.6182 s, Epoch 0,  Iteration 700, loss = 0.1880\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 22.7080 s, Epoch 1,  Iteration 0, loss = 0.3692\n",
      "Checking accuracy on validation set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "\n",
      "Elapsed 25.6804 s, Epoch 1,  Iteration 100, loss = 0.2824\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 28.6221 s, Epoch 1,  Iteration 200, loss = 0.2989\n",
      "Checking accuracy on validation set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "\n",
      "Elapsed 31.5696 s, Epoch 1,  Iteration 300, loss = 0.2705\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 34.5341 s, Epoch 1,  Iteration 400, loss = 0.3351\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 37.5190 s, Epoch 1,  Iteration 500, loss = 0.2068\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 40.4808 s, Epoch 1,  Iteration 600, loss = 0.2355\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 43.4373 s, Epoch 1,  Iteration 700, loss = 0.2172\n",
      "Checking accuracy on validation set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "\n",
      "Elapsed 45.4556 s, Epoch 2,  Iteration 0, loss = 0.2980\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 48.4191 s, Epoch 2,  Iteration 100, loss = 0.3243\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 51.3919 s, Epoch 2,  Iteration 200, loss = 0.3391\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 54.3130 s, Epoch 2,  Iteration 300, loss = 0.2833\n",
      "Checking accuracy on validation set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "\n",
      "Elapsed 57.2336 s, Epoch 2,  Iteration 400, loss = 0.3258\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 60.1645 s, Epoch 2,  Iteration 500, loss = 0.2545\n",
      "Checking accuracy on validation set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "\n",
      "Elapsed 63.0870 s, Epoch 2,  Iteration 600, loss = 0.5495\n",
      "Checking accuracy on validation set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "\n",
      "Elapsed 66.0228 s, Epoch 2,  Iteration 700, loss = 0.2511\n",
      "Checking accuracy on validation set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "\n",
      "Elapsed 68.0365 s, Epoch 3,  Iteration 0, loss = 0.2471\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 70.9842 s, Epoch 3,  Iteration 100, loss = 0.2532\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 73.9344 s, Epoch 3,  Iteration 200, loss = 0.2143\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 76.8723 s, Epoch 3,  Iteration 300, loss = 0.2336\n",
      "Checking accuracy on validation set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "\n",
      "Elapsed 79.8189 s, Epoch 3,  Iteration 400, loss = 0.2569\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 82.7402 s, Epoch 3,  Iteration 500, loss = 0.2153\n",
      "Checking accuracy on validation set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "\n",
      "Elapsed 85.6654 s, Epoch 3,  Iteration 600, loss = 0.2945\n",
      "Checking accuracy on validation set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "\n",
      "Elapsed 88.6021 s, Epoch 3,  Iteration 700, loss = 0.2152\n",
      "Checking accuracy on validation set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "\n",
      "Elapsed 90.6103 s, Epoch 4,  Iteration 0, loss = 0.2341\n",
      "Checking accuracy on validation set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "\n",
      "Elapsed 93.5365 s, Epoch 4,  Iteration 100, loss = 0.3131\n",
      "Checking accuracy on validation set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "\n",
      "Elapsed 96.4675 s, Epoch 4,  Iteration 200, loss = 0.2724\n",
      "Checking accuracy on validation set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "\n",
      "Elapsed 99.4002 s, Epoch 4,  Iteration 300, loss = 0.1906\n",
      "Checking accuracy on validation set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "\n",
      "Elapsed 102.3561 s, Epoch 4,  Iteration 400, loss = 0.2224\n",
      "Checking accuracy on validation set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "\n",
      "Elapsed 105.3017 s, Epoch 4,  Iteration 500, loss = 0.2882\n",
      "Checking accuracy on validation set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "\n",
      "Elapsed 108.2316 s, Epoch 4,  Iteration 600, loss = 0.2458\n",
      "Checking accuracy on validation set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "\n",
      "Elapsed 111.1641 s, Epoch 4,  Iteration 700, loss = 0.3110\n",
      "Checking accuracy on validation set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train this model\n",
    "learning_rate = 5e-7\n",
    "\n",
    "optimizer = optim.Adam(params=fix_model.parameters(), lr=learning_rate)\n",
    "#optimizer = optim.SGD(fix_model.parameters(), lr=learning_rate,momentum=0.9, nesterov=True)\n",
    "train_fixed_weight(fix_model, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Fixed Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 6876 / 10000 correct (68.76)\n",
      "\n",
      "Direct Fixed Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 6812 / 10000 correct (68.12)\n",
      "\n",
      "Original Floating Point Accuracy:\n",
      "Checking accuracy on test set\n",
      "Got 6812 / 10000 correct (68.12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Finetune Fixed Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, fix_model)\n",
    "print(\"\\nDirect Fixed Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, direct_fix_model)\n",
    "print(\"\\nOriginal Floating Point Accuracy:\")\n",
    "check_accuracy_part34(loader_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-198.2814,  255.2448,  117.6480, -207.7786, -189.1101],\n",
      "          [-103.1326,  251.6528,  -79.1741, -319.9017,  -12.8926],\n",
      "          [ 235.1852,   71.4552,  -19.8977, -286.5714,  216.7204],\n",
      "          [ 133.1982,  -64.3097, -264.7648,  126.2105,  -41.5371],\n",
      "          [  98.8290, -175.6736,  142.7939,  146.3576,  -59.8986]],\n",
      "\n",
      "         [[-251.8881, -104.8649, -257.0427,   79.2322,  175.4056],\n",
      "          [ -75.6155,   30.4536,  -73.4142,   42.4973,  243.2123],\n",
      "          [ 227.3108, -119.0568,   55.2801,   95.9392,  -11.9220],\n",
      "          [ 261.6974,   89.3977, -313.3453,  363.6068,  150.8462],\n",
      "          [ 256.3130, -276.7975, -355.8281,  401.6050,   18.7703]],\n",
      "\n",
      "         [[  46.9333,  237.2028,  129.9361,   20.4777,   22.2818],\n",
      "          [-229.5939,  169.2838,   20.6609,  111.7582,    4.7855],\n",
      "          [-165.5342,  -91.6710,   -9.8183,  -51.4141,  -62.1809],\n",
      "          [ 337.9961,  -96.6034, -162.7282,  -78.8396,   31.5714],\n",
      "          [  37.1734,   -9.6663, -127.2759, -253.4323, -148.9542]]],\n",
      "\n",
      "\n",
      "        [[[ 200.0011,  178.1546,  197.8615, -181.4164,  199.8802],\n",
      "          [ -80.7904,   -7.6254, -223.0728,  193.1431, -131.4914],\n",
      "          [ 120.2254, -161.8636, -104.9749,   34.5291,    3.0421],\n",
      "          [ -84.5481,   43.2944,  -60.8316, -177.8413, -133.3268],\n",
      "          [-234.0753, -250.2168,    0.4233, -205.7664, -198.2386]],\n",
      "\n",
      "         [[ 197.8664,  -85.3288,   88.8419,   -7.1772,  122.6109],\n",
      "          [ 187.1294,  212.3609,   46.6272,  197.5644,   27.8813],\n",
      "          [ -74.6110,  -50.5891,  -44.4429,   -8.8945,  248.0839],\n",
      "          [ 179.4999, -182.7308,  -94.7141, -289.2698, -122.5806],\n",
      "          [  72.4739,    3.0045, -163.3427,   16.5499, -176.3210]],\n",
      "\n",
      "         [[-186.9752,  -41.9504,  287.4312,   17.9488,  -97.8879],\n",
      "          [  15.3889,  136.5512,  -21.2025, -182.0746,  -12.4092],\n",
      "          [-118.4051,  -24.5181,  185.7738,  204.8595, -140.1131],\n",
      "          [-201.2928,  218.2913,  323.8619,   87.9350,  -63.2505],\n",
      "          [   9.7876,  152.2811,   56.2970,  142.8787, -182.1942]]],\n",
      "\n",
      "\n",
      "        [[[ 127.6880, -166.3058,  111.8529,    5.9529,   -9.7691],\n",
      "          [  51.2283,  -61.8856,  -91.1051,    3.3456,  -13.7131],\n",
      "          [ 211.1655,  -52.2234,   44.1763,  125.6279,   39.0781],\n",
      "          [-336.7621,   51.2611,   14.7283,  331.8279, -236.3543],\n",
      "          [-117.2514,  143.3544, -158.8422,  -69.3448,  -53.5928]],\n",
      "\n",
      "         [[  54.9081,   -8.2544,  213.2658, -176.9472,  -94.6802],\n",
      "          [  98.8138,  117.6340, -145.9907,   31.9177,  270.2360],\n",
      "          [ -60.2433, -344.0127,  225.4430,  -73.2903,  -71.7704],\n",
      "          [-182.8486,  210.1451,   79.4522,  -99.5350, -285.4413],\n",
      "          [ -18.9431,  181.1265,   14.8307,  309.0965,   19.7573]],\n",
      "\n",
      "         [[  69.6823,  162.8738, -247.8393,   -2.1021, -121.6483],\n",
      "          [  16.8810, -263.5927, -378.9791, -172.9823,  208.6763],\n",
      "          [-169.5744, -167.7555,  195.3461,  544.6024,  197.5276],\n",
      "          [  -0.0747, -184.6879,  204.4107,  377.2695, -626.4212],\n",
      "          [-125.8693,  403.3427, -185.3322, -148.5173,  123.6597]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -60.1116,  -45.9625,   25.2653,  127.1687,  -13.2040],\n",
      "          [ 274.6117,  -16.7152,  -68.2330,  229.0948, -101.0548],\n",
      "          [ -55.4914,  242.7155,  154.1629,   -7.1092, -311.1519],\n",
      "          [-203.2455,    7.6553,  103.3045, -102.5223,  -13.1743],\n",
      "          [  24.3952,  219.3768,  291.4542, -575.0469,   57.5187]],\n",
      "\n",
      "         [[ 103.5446, -236.9959, -414.9496,  -31.8814,   59.6859],\n",
      "          [ 106.9593,  206.9286, -213.5536, -241.2837, -295.5966],\n",
      "          [ -82.3217,  103.7436,  169.4003,  -35.1727, -175.2439],\n",
      "          [  86.7115, -264.4635,  261.9442,   65.5402,   49.7970],\n",
      "          [ -62.4245,   13.0723,  235.2114,   66.1526,  231.7780]],\n",
      "\n",
      "         [[ 192.5788,  -18.7022, -165.1956,  -53.5885, -273.5742],\n",
      "          [-128.0382,  333.2508,  104.1888, -201.0034, -312.9747],\n",
      "          [  33.0576,  300.1584,  256.5858,  116.3538, -205.6156],\n",
      "          [-252.3382,  273.3961,   37.3994,  187.5786,   -2.2959],\n",
      "          [-295.7195, -125.6107,   84.7048,   15.5678,  155.3142]]],\n",
      "\n",
      "\n",
      "        [[[-180.2353,  216.6201, -219.7980,   -1.4402,   33.5088],\n",
      "          [ -19.9098, -162.6290,   99.6278,   62.7488, -176.3773],\n",
      "          [-102.5619,   48.3477,  -91.8632, -145.4081,  173.2540],\n",
      "          [   7.5542,   89.0751,  -13.0288,  -46.3351,  245.1204],\n",
      "          [ -57.3161,   88.9759,  136.9827, -364.2197,   23.1624]],\n",
      "\n",
      "         [[-177.3201, -117.7773,  320.3268,  135.6563,  114.6327],\n",
      "          [   2.0521,   66.0593,   85.4411,   99.6092,  158.1460],\n",
      "          [-142.8582,   -6.5567,   -0.2192, -146.9066,  342.7377],\n",
      "          [-104.0831,   22.6219, -202.4072, -112.9146,    2.9820],\n",
      "          [  -2.3650,  -27.6454, -173.4029,   79.4413, -131.7020]],\n",
      "\n",
      "         [[  -5.1178, -231.5542,   39.4483,  102.4851,   35.0907],\n",
      "          [-278.5711, -141.4806,  258.4470, -222.2273,    2.5049],\n",
      "          [ 103.0411, -257.9835,    4.4211,  293.4292,  306.2906],\n",
      "          [  -0.9418, -151.8677,   47.8776, -157.4377,  336.6356],\n",
      "          [ -61.4115,  -94.1809,   50.9843,   17.9062,  198.2281]]],\n",
      "\n",
      "\n",
      "        [[[-264.8149,  -12.5525,  -18.1471,   33.2687, -238.6851],\n",
      "          [ 155.0648,  -33.4528,   50.4867,  184.2129,   36.3585],\n",
      "          [ -64.1029, -227.1499,    1.1901, -150.7746,  -82.5157],\n",
      "          [ 540.8966, -126.2673, -149.1435, -142.0134,   62.5688],\n",
      "          [ -41.9019,  190.6466,   -1.9666,  218.7408,  136.9765]],\n",
      "\n",
      "         [[  26.0985,  -33.0662,  581.6558,  181.1683, -114.8004],\n",
      "          [-475.5923,  311.4924,   32.8967,  146.4288,   85.3521],\n",
      "          [ -70.5550, -169.3291,  -56.5608,    2.4743,  111.6928],\n",
      "          [  28.6486, -125.6425,  145.5541, -186.4872, -409.4834],\n",
      "          [-114.7094,  131.0113, -345.2665,  -30.5687,   58.4738]],\n",
      "\n",
      "         [[ -94.2657, -128.6212,  300.4352, -262.8008, -185.8249],\n",
      "          [-273.4633,   99.4216,  424.5159,  -44.7913,   55.3303],\n",
      "          [ 211.6395,   61.2101,   42.2692, -197.0287,   20.9500],\n",
      "          [ 108.1189,  285.1844,  -97.6211, -163.8457,  -90.1795],\n",
      "          [ -57.9451,  115.0400,  -23.7941,  108.8186,  -21.9791]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(direct_fix_model.module.state_dict()['conv1.weight']*1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
